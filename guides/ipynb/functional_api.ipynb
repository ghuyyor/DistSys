{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLblXjPe-eBC"
      },
      "source": [
        "# The Functional API\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/03/01<br>\n",
        "**Last modified:** 2023/06/25<br>\n",
        "**Description:** Complete guide to the functional API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_h7MRCF-eBF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot2dlOn2-eBH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goi-UmIn-eBJ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The Keras *functional API* is a way to create models that are more flexible\n",
        "than the `keras.Sequential` API. The functional API can handle models\n",
        "with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
        "\n",
        "The main idea is that a deep learning model is usually\n",
        "a directed acyclic graph (DAG) of layers.\n",
        "So the functional API is a way to build *graphs of layers*.\n",
        "\n",
        "Consider the following model:\n",
        "\n",
        "<div class=\"k-default-codeblock\">\n",
        "```\n",
        "(input: 784-dimensional vectors)\n",
        "       ↧\n",
        "[Dense (64 units, relu activation)]\n",
        "       ↧\n",
        "[Dense (64 units, relu activation)]\n",
        "       ↧\n",
        "[Dense (10 units, softmax activation)]\n",
        "       ↧\n",
        "(output: logits of a probability distribution over 10 classes)\n",
        "```\n",
        "</div>\n",
        "\n",
        "This is a basic graph with three layers.\n",
        "To build this model using the functional API, start by creating an input node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FEUSeOA-eBL"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(784,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pv08Sy7-eBM"
      },
      "source": [
        "The shape of the data is set as a 784-dimensional vector.\n",
        "The batch size is always omitted since only the shape of each sample is specified.\n",
        "\n",
        "If, for example, you have an image input with a shape of `(32, 32, 3)`,\n",
        "you would use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE9VSoo3-eBM"
      },
      "outputs": [],
      "source": [
        "# Just for demonstration purposes.\n",
        "img_inputs = keras.Input(shape=(32, 32, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AqmrlNr-eBN"
      },
      "source": [
        "The `inputs` that is returned contains information about the shape and `dtype`\n",
        "of the input data that you feed to your model.\n",
        "Here's the shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q44jy-ed-eBP"
      },
      "outputs": [],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB72sRmJ-eBR"
      },
      "source": [
        "Here's the dtype:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg4lpmz6-eBS"
      },
      "outputs": [],
      "source": [
        "inputs.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrJ2-INw-eBU"
      },
      "source": [
        "You create a new node in the graph of layers by calling a layer on this `inputs`\n",
        "object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36d778Q4-eBU"
      },
      "outputs": [],
      "source": [
        "dense = layers.Dense(64, activation=\"relu\")\n",
        "x = dense(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksngBPcY-eBV"
      },
      "source": [
        "The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer\n",
        "you created.\n",
        "You're \"passing\" the inputs to the `dense` layer, and you get `x` as the output.\n",
        "\n",
        "Let's add a few more layers to the graph of layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQmYSkjo-eBV"
      },
      "outputs": [],
      "source": [
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10)(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdyRufSy-eBV"
      },
      "source": [
        "At this point, you can create a `Model` by specifying its inputs and outputs\n",
        "in the graph of layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb8_xy5t-eBW"
      },
      "outputs": [],
      "source": [
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZfElH2X-eBW"
      },
      "source": [
        "Let's check out what the model summary looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL4Ljv0v-eBX"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIDS3zT6-eBX"
      },
      "source": [
        "You can also plot the model as a graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2VFj4Ku-eBY"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"my_first_model.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy6pPEw2-eBY"
      },
      "source": [
        "And, optionally, display the input and output shapes of each layer\n",
        "in the plotted graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6FZqlzA-eBZ"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhH7rkgC-eBZ"
      },
      "source": [
        "This figure and the code are almost identical. In the code version,\n",
        "the connection arrows are replaced by the call operation.\n",
        "\n",
        "A \"graph of layers\" is an intuitive mental image for a deep learning model,\n",
        "and the functional API is a way to create models that closely mirrors this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hotlrqb-eBa"
      },
      "source": [
        "## Training, evaluation, and inference\n",
        "\n",
        "Training, evaluation, and inference work exactly in the same way for models\n",
        "built using the functional API as for `Sequential` models.\n",
        "\n",
        "The `Model` class offers a built-in training loop (the `fit()` method)\n",
        "and a built-in evaluation loop (the `evaluate()` method). Note\n",
        "that you can easily customize these loops to implement your own training routines.\n",
        "See also the guides on customizing what happens in `fit()`:\n",
        "\n",
        "- [Writing a custom train step with TensorFlow](/guides/custom_train_step_in_tensorflow/)\n",
        "- [Writing a custom train step with JAX](/guides/custom_train_step_in_jax/)\n",
        "- [Writing a custom train step with PyTorch](/guides/custom_train_step_in_torch/)\n",
        "\n",
        "Here, load the MNIST image data, reshape it into vectors,\n",
        "fit the model on the data (while monitoring performance on a validation split),\n",
        "then evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB2V9CFu-eBa"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
        "\n",
        "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzDu0v5D-eBb"
      },
      "source": [
        "For further reading, see the\n",
        "[training and evaluation](/guides/training_with_built_in_methods/) guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By06cuDZ-eBb"
      },
      "source": [
        "## Save and serialize\n",
        "\n",
        "Saving the model and serialization work the same way for models built using\n",
        "the functional API as they do for `Sequential` models. The standard way\n",
        "to save a functional model is to call `model.save()`\n",
        "to save the entire model as a single file. You can later recreate the same model\n",
        "from this file, even if the code that built the model is no longer available.\n",
        "\n",
        "This saved file includes the:\n",
        "- model architecture\n",
        "- model weight values (that were learned during training)\n",
        "- model training config, if any (as passed to `compile()`)\n",
        "- optimizer and its state, if any (to restart training where you left off)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_9dZu8f-eBc"
      },
      "outputs": [],
      "source": [
        "model.save(\"my_model.keras\")\n",
        "del model\n",
        "# Recreate the exact same model purely from the file:\n",
        "model = keras.models.load_model(\"my_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO7gXok_-eBc"
      },
      "source": [
        "For details, read the model [serialization & saving](/guides/serialization_and_saving/) guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5pkmj1f-eBc"
      },
      "source": [
        "## Use the same graph of layers to define multiple models\n",
        "\n",
        "In the functional API, models are created by specifying their inputs\n",
        "and outputs in a graph of layers. That means that a single\n",
        "graph of layers can be used to generate multiple models.\n",
        "\n",
        "In the example below, you use the same stack of layers to instantiate two models:\n",
        "an `encoder` model that turns image inputs into 16-dimensional vectors,\n",
        "and an end-to-end `autoencoder` model for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_GBndn--eBd"
      },
      "outputs": [],
      "source": [
        "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_ZadMs8-eBd"
      },
      "source": [
        "Here, the decoding architecture is strictly symmetrical\n",
        "to the encoding architecture, so the output shape is the same as\n",
        "the input shape `(28, 28, 1)`.\n",
        "\n",
        "The reverse of a `Conv2D` layer is a `Conv2DTranspose` layer,\n",
        "and the reverse of a `MaxPooling2D` layer is an `UpSampling2D` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxg1WZ6J-eBe"
      },
      "source": [
        "## All models are callable, just like layers\n",
        "\n",
        "You can treat any model as if it were a layer by invoking it on an `Input` or\n",
        "on the output of another layer. By calling a model you aren't just reusing\n",
        "the architecture of the model, you're also reusing its weights.\n",
        "\n",
        "To see this in action, here's a different take on the autoencoder example that\n",
        "creates an encoder model, a decoder model, and chains them in two calls\n",
        "to obtain the autoencoder model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGC9FBZa-eBe"
      },
      "outputs": [],
      "source": [
        "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
        "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
        "\n",
        "decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "decoder.summary()\n",
        "\n",
        "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
        "encoded_img = encoder(autoencoder_input)\n",
        "decoded_img = decoder(encoded_img)\n",
        "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOx0J-FU-eBn"
      },
      "source": [
        "As you can see, the model can be nested: a model can contain sub-models\n",
        "(since a model is just like a layer).\n",
        "A common use case for model nesting is *ensembling*.\n",
        "For example, here's how to ensemble a set of models into a single model\n",
        "that averages their predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hF0eeSJ-eBo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model():\n",
        "    inputs = keras.Input(shape=(128,))\n",
        "    outputs = layers.Dense(1)(inputs)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model1 = get_model()\n",
        "model2 = get_model()\n",
        "model3 = get_model()\n",
        "\n",
        "inputs = keras.Input(shape=(128,))\n",
        "y1 = model1(inputs)\n",
        "y2 = model2(inputs)\n",
        "y3 = model3(inputs)\n",
        "outputs = layers.average([y1, y2, y3])\n",
        "ensemble_model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyMOM80-eBp"
      },
      "source": [
        "## Manipulate complex graph topologies\n",
        "\n",
        "### Models with multiple inputs and outputs\n",
        "\n",
        "The functional API makes it easy to manipulate multiple inputs and outputs.\n",
        "This cannot be handled with the `Sequential` API.\n",
        "\n",
        "For example, if you're building a system for ranking customer issue tickets by\n",
        "priority and routing them to the correct department,\n",
        "then the model will have three inputs:\n",
        "\n",
        "- the title of the ticket (text input),\n",
        "- the text body of the ticket (text input), and\n",
        "- any tags added by the user (categorical input)\n",
        "\n",
        "This model will have two outputs:\n",
        "\n",
        "- the priority score between 0 and 1 (scalar sigmoid output), and\n",
        "- the department that should handle the ticket (softmax output\n",
        "over the set of departments).\n",
        "\n",
        "You can build this model in a few lines with the functional API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOPefv4e-eBq"
      },
      "outputs": [],
      "source": [
        "num_tags = 12  # Number of unique issue tags\n",
        "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
        "num_departments = 4  # Number of departments for predictions\n",
        "\n",
        "title_input = keras.Input(\n",
        "    shape=(None,), name=\"title\"\n",
        ")  # Variable-length sequence of ints\n",
        "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
        "tags_input = keras.Input(\n",
        "    shape=(num_tags,), name=\"tags\"\n",
        ")  # Binary vectors of size `num_tags`\n",
        "\n",
        "# Embed each word in the title into a 64-dimensional vector\n",
        "title_features = layers.Embedding(num_words, 64)(title_input)\n",
        "# Embed each word in the text into a 64-dimensional vector\n",
        "body_features = layers.Embedding(num_words, 64)(body_input)\n",
        "\n",
        "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
        "title_features = layers.LSTM(128)(title_features)\n",
        "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
        "body_features = layers.LSTM(32)(body_features)\n",
        "\n",
        "# Merge all available features into a single large vector via concatenation\n",
        "x = layers.concatenate([title_features, body_features, tags_input])\n",
        "\n",
        "# Stick a logistic regression for priority prediction on top of the features\n",
        "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
        "# Stick a department classifier on top of the features\n",
        "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
        "\n",
        "# Instantiate an end-to-end model predicting both priority and department\n",
        "model = keras.Model(\n",
        "    inputs=[title_input, body_input, tags_input],\n",
        "    outputs={\"priority\": priority_pred, \"department\": department_pred},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-aMtfqw-eBr"
      },
      "source": [
        "Now plot the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXOYRSZV-eBr"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w1EdRxW-eBs"
      },
      "source": [
        "When compiling this model, you can assign different losses to each output.\n",
        "You can even assign different weights to each loss -- to modulate\n",
        "their contribution to the total training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKiNgbFk-eBt"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[\n",
        "        keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    ],\n",
        "    loss_weights=[1.0, 0.2],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qIBNM04-eBu"
      },
      "source": [
        "Since the output layers have different names, you could also specify\n",
        "the losses and loss weights with the corresponding layer names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6m-t2TM-eBu"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={\n",
        "        \"priority\": keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        \"department\": keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    },\n",
        "    loss_weights={\"priority\": 1.0, \"department\": 0.2},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIwuf-nI-eBv"
      },
      "source": [
        "Train the model by passing lists of NumPy arrays of inputs and targets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPz_hGtZ-eBv"
      },
      "outputs": [],
      "source": [
        "# Dummy input data\n",
        "title_data = np.random.randint(num_words, size=(1280, 12))\n",
        "body_data = np.random.randint(num_words, size=(1280, 100))\n",
        "tags_data = np.random.randint(2, size=(1280, num_tags)).astype(\"float32\")\n",
        "\n",
        "# Dummy target data\n",
        "priority_targets = np.random.random(size=(1280, 1))\n",
        "dept_targets = np.random.randint(2, size=(1280, num_departments))\n",
        "\n",
        "model.fit(\n",
        "    {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},\n",
        "    {\"priority\": priority_targets, \"department\": dept_targets},\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5lPzDpK-eBw"
      },
      "source": [
        "When calling fit with a `Dataset` object, it should yield either a\n",
        "tuple of lists like `([title_data, body_data, tags_data], [priority_targets, dept_targets])`\n",
        "or a tuple of dictionaries like\n",
        "`({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})`.\n",
        "\n",
        "For more detailed explanation, refer to the\n",
        "[training and evaluation](/guides/training_with_built_in_methods/) guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymNxik-C-eBw"
      },
      "source": [
        "### A toy ResNet model\n",
        "\n",
        "In addition to models with multiple inputs and outputs,\n",
        "the functional API makes it easy to manipulate non-linear connectivity\n",
        "topologies -- these are models with layers that are not connected sequentially,\n",
        "which the `Sequential` API cannot handle.\n",
        "\n",
        "A common use case for this is residual connections.\n",
        "Let's build a toy ResNet model for CIFAR10 to demonstrate this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29gi3FDA-eBy"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3), name=\"img\")\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
        "block_1_output = layers.MaxPooling2D(3)(x)\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "block_2_output = layers.add([x, block_1_output])\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "block_3_output = layers.add([x, block_2_output])\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10)(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs, name=\"toy_resnet\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UblsxSYs-eBz"
      },
      "source": [
        "Plot the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf2xcvVT-eBz"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"mini_resnet.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IImpbn2o-eB0"
      },
      "source": [
        "Now train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qd2mG7a-eB0"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "# We restrict the data to the first 1000 samples so as to limit execution time\n",
        "# on Colab. Try to train on the entire dataset until convergence!\n",
        "model.fit(\n",
        "    x_train[:1000],\n",
        "    y_train[:1000],\n",
        "    batch_size=64,\n",
        "    epochs=1,\n",
        "    validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzoKrOox-eB1"
      },
      "source": [
        "## Shared layers\n",
        "\n",
        "Another good use for the functional API are models that use *shared layers*.\n",
        "Shared layers are layer instances that are reused multiple times in the same model --\n",
        "they learn features that correspond to multiple paths in the graph-of-layers.\n",
        "\n",
        "Shared layers are often used to encode inputs from similar spaces\n",
        "(say, two different pieces of text that feature similar vocabulary).\n",
        "They enable sharing of information across these different inputs,\n",
        "and they make it possible to train such a model on less data.\n",
        "If a given word is seen in one of the inputs,\n",
        "that will benefit the processing of all inputs that pass through the shared layer.\n",
        "\n",
        "To share a layer in the functional API, call the same layer instance multiple times.\n",
        "For instance, here's an `Embedding` layer shared across two different text inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlWU6K-0-eB2"
      },
      "outputs": [],
      "source": [
        "# Embedding for 1000 unique words mapped to 128-dimensional vectors\n",
        "shared_embedding = layers.Embedding(1000, 128)\n",
        "\n",
        "# Variable-length sequence of integers\n",
        "text_input_a = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# Variable-length sequence of integers\n",
        "text_input_b = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# Reuse the same layer to encode both inputs\n",
        "encoded_input_a = shared_embedding(text_input_a)\n",
        "encoded_input_b = shared_embedding(text_input_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo9E9KPh-eB2"
      },
      "source": [
        "## Extract and reuse nodes in the graph of layers\n",
        "\n",
        "Because the graph of layers you are manipulating is a static data structure,\n",
        "it can be accessed and inspected. And this is how you are able to plot\n",
        "functional models as images.\n",
        "\n",
        "This also means that you can access the activations of intermediate layers\n",
        "(\"nodes\" in the graph) and reuse them elsewhere --\n",
        "which is very useful for something like feature extraction.\n",
        "\n",
        "Let's look at an example. This is a VGG19 model with weights pretrained on ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VWjVfSb-eB3"
      },
      "outputs": [],
      "source": [
        "vgg19 = keras.applications.VGG19()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_TrjYEv-eB3"
      },
      "source": [
        "And these are the intermediate activations of the model,\n",
        "obtained by querying the graph data structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slkkkHqK-eB3"
      },
      "outputs": [],
      "source": [
        "features_list = [layer.output for layer in vgg19.layers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDub7OrK-eB4"
      },
      "source": [
        "Use these features to create a new feature-extraction model that returns\n",
        "the values of the intermediate layer activations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzhu7j9v-eB4"
      },
      "outputs": [],
      "source": [
        "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
        "\n",
        "img = np.random.random((1, 224, 224, 3)).astype(\"float32\")\n",
        "extracted_features = feat_extraction_model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8k7DLN0-eB5"
      },
      "source": [
        "This comes in handy for tasks like\n",
        "[neural style transfer](https://keras.io/examples/generative/neural_style_transfer/),\n",
        "among other things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIkIbZJ-eB5"
      },
      "source": [
        "## Extend the API using custom layers\n",
        "\n",
        "`keras` includes a wide range of built-in layers, for example:\n",
        "\n",
        "- Convolutional layers: `Conv1D`, `Conv2D`, `Conv3D`, `Conv2DTranspose`\n",
        "- Pooling layers: `MaxPooling1D`, `MaxPooling2D`, `MaxPooling3D`, `AveragePooling1D`\n",
        "- RNN layers: `GRU`, `LSTM`, `ConvLSTM2D`\n",
        "- `BatchNormalization`, `Dropout`, `Embedding`, etc.\n",
        "\n",
        "But if you don't find what you need, it's easy to extend the API by creating\n",
        "your own layers. All layers subclass the `Layer` class and implement:\n",
        "\n",
        "- `call` method, that specifies the computation done by the layer.\n",
        "- `build` method, that creates the weights of the layer (this is just a style\n",
        "convention since you can create weights in `__init__`, as well).\n",
        "\n",
        "To learn more about creating layers from scratch, read\n",
        "[custom layers and models](/guides/making_new_layers_and_models_via_subclassing) guide.\n",
        "\n",
        "The following is a basic implementation of `keras.layers.Dense`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg16xRe9-eB6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomDense(layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return ops.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "inputs = keras.Input((4,))\n",
        "outputs = CustomDense(10)(inputs)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHJdpTS-eB6"
      },
      "source": [
        "For serialization support in your custom layer, define a `get_config()`\n",
        "method that returns the constructor arguments of the layer instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdW3Gc1k-eB6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomDense(layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return ops.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"units\": self.units}\n",
        "\n",
        "\n",
        "inputs = keras.Input((4,))\n",
        "outputs = CustomDense(10)(inputs)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "config = model.get_config()\n",
        "\n",
        "new_model = keras.Model.from_config(config, custom_objects={\"CustomDense\": CustomDense})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPpgoU1-eB7"
      },
      "source": [
        "Optionally, implement the class method `from_config(cls, config)` which is used\n",
        "when recreating a layer instance given its config dictionary.\n",
        "The default implementation of `from_config` is:\n",
        "\n",
        "```python\n",
        "def from_config(cls, config):\n",
        "  return cls(**config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZkUEN_L-eB7"
      },
      "source": [
        "## When to use the functional API\n",
        "\n",
        "Should you use the Keras functional API to create a new model,\n",
        "or just subclass the `Model` class directly? In general, the functional API\n",
        "is higher-level, easier and safer, and has a number of\n",
        "features that subclassed models do not support.\n",
        "\n",
        "However, model subclassing provides greater flexibility when building models\n",
        "that are not easily expressible as directed acyclic graphs of layers.\n",
        "For example, you could not implement a Tree-RNN with the functional API\n",
        "and would have to subclass `Model` directly.\n",
        "\n",
        "For an in-depth look at the differences between the functional API and\n",
        "model subclassing, read\n",
        "[What are Symbolic and Imperative APIs in TensorFlow 2.0?](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
        "\n",
        "### Functional API strengths:\n",
        "\n",
        "The following properties are also true for Sequential models\n",
        "(which are also data structures), but are not true for subclassed models\n",
        "(which are Python bytecode, not data structures).\n",
        "\n",
        "#### Less verbose\n",
        "\n",
        "There is no `super().__init__(...)`, no `def call(self, ...):`, etc.\n",
        "\n",
        "Compare:\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(32,))\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "outputs = layers.Dense(10)(x)\n",
        "mlp = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "With the subclassed version:\n",
        "\n",
        "```python\n",
        "class MLP(keras.Model):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.dense_1 = layers.Dense(64, activation='relu')\n",
        "    self.dense_2 = layers.Dense(10)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_1(inputs)\n",
        "    return self.dense_2(x)\n",
        "\n",
        "# Instantiate the model.\n",
        "mlp = MLP()\n",
        "# Necessary to create the model's state.\n",
        "# The model doesn't have a state until it's called at least once.\n",
        "_ = mlp(ops.zeros((1, 32)))\n",
        "```\n",
        "\n",
        "#### Model validation while defining its connectivity graph\n",
        "\n",
        "In the functional API, the input specification (shape and dtype) is created\n",
        "in advance (using `Input`). Every time you call a layer,\n",
        "the layer checks that the specification passed to it matches its assumptions,\n",
        "and it will raise a helpful error message if not.\n",
        "\n",
        "This guarantees that any model you can build with the functional API will run.\n",
        "All debugging -- other than convergence-related debugging --\n",
        "happens statically during the model construction and not at execution time.\n",
        "This is similar to type checking in a compiler.\n",
        "\n",
        "#### A functional model is plottable and inspectable\n",
        "\n",
        "You can plot the model as a graph, and you can easily access intermediate nodes\n",
        "in this graph. For example, to extract and reuse the activations of intermediate\n",
        "layers (as seen in a previous example):\n",
        "\n",
        "```python\n",
        "features_list = [layer.output for layer in vgg19.layers]\n",
        "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
        "```\n",
        "\n",
        "#### A functional model can be serialized or cloned\n",
        "\n",
        "Because a functional model is a data structure rather than a piece of code,\n",
        "it is safely serializable and can be saved as a single file\n",
        "that allows you to recreate the exact same model\n",
        "without having access to any of the original code.\n",
        "See the [serialization & saving guide](/guides/serialization_and_saving/).\n",
        "\n",
        "To serialize a subclassed model, it is necessary for the implementer\n",
        "to specify a `get_config()`\n",
        "and `from_config()` method at the model level.\n",
        "\n",
        "\n",
        "### Functional API weakness:\n",
        "\n",
        "#### It does not support dynamic architectures\n",
        "\n",
        "The functional API treats models as DAGs of layers.\n",
        "This is true for most deep learning architectures, but not all -- for example,\n",
        "recursive networks or Tree RNNs do not follow this assumption and cannot\n",
        "be implemented in the functional API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ulpaKy-eB8"
      },
      "source": [
        "## Mix-and-match API styles\n",
        "\n",
        "Choosing between the functional API or Model subclassing isn't a\n",
        "binary decision that restricts you into one category of models.\n",
        "All models in the `keras` API can interact with each other, whether they're\n",
        "`Sequential` models, functional models, or subclassed models that are written\n",
        "from scratch.\n",
        "\n",
        "You can always use a functional model or `Sequential` model\n",
        "as part of a subclassed model or layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tcqN_LY-eB8"
      },
      "outputs": [],
      "source": [
        "units = 32\n",
        "timesteps = 10\n",
        "input_dim = 5\n",
        "\n",
        "# Define a Functional model\n",
        "inputs = keras.Input((None, units))\n",
        "x = layers.GlobalAveragePooling1D()(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "class CustomRNN(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n",
        "        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n",
        "        # Our previously-defined Functional model\n",
        "        self.classifier = model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = []\n",
        "        state = ops.zeros(shape=(inputs.shape[0], self.units))\n",
        "        for t in range(inputs.shape[1]):\n",
        "            x = inputs[:, t, :]\n",
        "            h = self.projection_1(x)\n",
        "            y = h + self.projection_2(state)\n",
        "            state = y\n",
        "            outputs.append(y)\n",
        "        features = ops.stack(outputs, axis=1)\n",
        "        print(features.shape)\n",
        "        return self.classifier(features)\n",
        "\n",
        "\n",
        "rnn_model = CustomRNN()\n",
        "_ = rnn_model(ops.zeros((1, timesteps, input_dim)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBih8Prl-eB9"
      },
      "source": [
        "You can use any subclassed layer or model in the functional API\n",
        "as long as it implements a `call` method that follows one of the following patterns:\n",
        "\n",
        "- `call(self, inputs, **kwargs)` --\n",
        "Where `inputs` is a tensor or a nested structure of tensors (e.g. a list of tensors),\n",
        "and where `**kwargs` are non-tensor arguments (non-inputs).\n",
        "- `call(self, inputs, training=None, **kwargs)` --\n",
        "Where `training` is a boolean indicating whether the layer should behave\n",
        "in training mode and inference mode.\n",
        "- `call(self, inputs, mask=None, **kwargs)` --\n",
        "Where `mask` is a boolean mask tensor (useful for RNNs, for instance).\n",
        "- `call(self, inputs, training=None, mask=None, **kwargs)` --\n",
        "Of course, you can have both masking and training-specific behavior at the same time.\n",
        "\n",
        "Additionally, if you implement the `get_config` method on your custom Layer or model,\n",
        "the functional models you create will still be serializable and cloneable.\n",
        "\n",
        "Here's a quick example of a custom RNN, written from scratch,\n",
        "being used in a functional model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxtMIDqv-eB9"
      },
      "outputs": [],
      "source": [
        "units = 32\n",
        "timesteps = 10\n",
        "input_dim = 5\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "class CustomRNN(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n",
        "        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n",
        "        self.classifier = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = []\n",
        "        state = ops.zeros(shape=(inputs.shape[0], self.units))\n",
        "        for t in range(inputs.shape[1]):\n",
        "            x = inputs[:, t, :]\n",
        "            h = self.projection_1(x)\n",
        "            y = h + self.projection_2(state)\n",
        "            state = y\n",
        "            outputs.append(y)\n",
        "        features = ops.stack(outputs, axis=1)\n",
        "        return self.classifier(features)\n",
        "\n",
        "\n",
        "# Note that you specify a static batch size for the inputs with the `batch_shape`\n",
        "# arg, because the inner computation of `CustomRNN` requires a static batch size\n",
        "# (when you create the `state` zeros tensor).\n",
        "inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))\n",
        "x = layers.Conv1D(32, 3)(inputs)\n",
        "outputs = CustomRNN()(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "rnn_model = CustomRNN()\n",
        "_ = rnn_model(ops.zeros((1, 10, 5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fefba95"
      },
      "source": [
        "# Task\n",
        "Design an integrated distributed architecture with multiple input and output nodes that also serve as sources of learning, where a central system updates itself based on these inputs and provides recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f225c088"
      },
      "source": [
        "## Define the architecture components\n",
        "\n",
        "### Subtask:\n",
        "Clearly define the different parts of the system, including the input nodes, output nodes, and the central learning/recommendation system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65569512"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the conceptual roles and responsibilities of the input nodes, output nodes, and the central learning/recommendation system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80f2048"
      },
      "source": [
        "# 1. Input Nodes:\n",
        "# Conceptual Role: Sources of data and learning signals.\n",
        "# Responsibilities:\n",
        "# - Collect data from their specific environment or domain.\n",
        "# - Preprocess data into a format suitable for the central system.\n",
        "# - Transmit processed data to the central learning/recommendation system.\n",
        "# - Potentially perform initial feature extraction or filtering.\n",
        "\n",
        "# 2. Output Nodes:\n",
        "# Conceptual Role: Consumers of recommendations and potentially sources of feedback/learning.\n",
        "# Responsibilities:\n",
        "# - Receive recommendations from the central learning/recommendation system.\n",
        "# - Act upon the recommendations.\n",
        "# - Potentially provide feedback on the effectiveness of the recommendations back to the central system.\n",
        "# - Potentially provide additional data or context based on their actions.\n",
        "\n",
        "# 3. Central Learning/Recommendation System:\n",
        "# Conceptual Role: The core intelligence of the system, responsible for learning patterns and generating recommendations.\n",
        "# Responsibilities:\n",
        "# - Receive and integrate data from multiple input nodes.\n",
        "# - Train and update internal models based on the received data.\n",
        "# - Generate recommendations for the output nodes based on the learned patterns and potentially real-time input.\n",
        "# - Manage and coordinate the learning process across different data sources.\n",
        "# - Potentially handle feedback from output nodes to improve future recommendations."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75cc6c68"
      },
      "source": [
        "## Design the data flow\n",
        "\n",
        "### Subtask:\n",
        "Map out how data will flow from the input nodes to the central system, how the central system will process this data for learning and recommendations, and how recommendations will be delivered to the output nodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc67e72"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the initial data flow from the input nodes to the central learning/recommendation system, the processing and learning within the central system, the recommendation generation, the delivery of recommendations to output nodes, and the feedback loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b2ada66"
      },
      "source": [
        "# 1. Data flow from input nodes to the central system:\n",
        "# Data Type: Could include various data types depending on the domain, e.g., sensor data (time series), transaction data, user interaction logs, text data, image data.\n",
        "# Data Format: Data should be in a structured format, such as JSON, Protobuf, or a standardized data schema agreed upon by the input nodes and the central system.\n",
        "# Protocols: Secure and reliable communication protocols are essential. Examples include gRPC for high-performance, bidirectional streaming, or RESTful APIs for simpler request/response interactions. Message queues (e.g., Kafka, RabbitMQ) can be used for asynchronous data ingestion and decoupling.\n",
        "\n",
        "# 2. Processing and integration in the central system:\n",
        "# Data Integration: The central system receives data from multiple input nodes, which might have different formats, frequencies, and semantics. Data integration involves data validation, cleaning, transformation, and merging into a unified representation.\n",
        "# Feature Engineering: Extracting relevant features from the integrated data that are suitable for the learning models. This might involve aggregation, normalization, and creating new features.\n",
        "# Learning Process: The central system employs machine learning models (e.g., deep learning models, collaborative filtering, reinforcement learning) that can learn from the diverse data sources. This could involve a single model trained on all data or multiple specialized models. The learning process updates the model parameters based on the incoming data, potentially in a continuous or batch manner.\n",
        "\n",
        "# 3. Recommendation generation:\n",
        "# The central system uses the learned models to generate recommendations. The nature of recommendations depends on the application (e.g., product recommendations, actions to take, parameters to adjust).\n",
        "# Inference: The learned models take the processed input data (potentially real-time or batch) and generate predictions or scores.\n",
        "# Recommendation Logic: Based on the model outputs, a recommendation logic formulates the final recommendations. This might involve ranking, filtering, or combining outputs from multiple models.\n",
        "\n",
        "# 4. Recommendation delivery to output nodes:\n",
        "# Data Format: Recommendations should be in a clear and easily parsable format for the output nodes, similar to the input data formats (e.g., JSON).\n",
        "# Delivery Mechanism: Recommendations can be delivered via similar protocols as data ingestion, such as gRPC or RESTful APIs. Push mechanisms (e.g., WebSockets, server-sent events) could be used for real-time recommendations.\n",
        "# Routing: The central system needs to route the correct recommendations to the appropriate output nodes.\n",
        "\n",
        "# 5. Feedback loop from output nodes:\n",
        "# Feedback Type: Output nodes can provide explicit feedback (e.g., ratings, acceptance/rejection of recommendations) or implicit feedback (e.g., actions taken after receiving a recommendation).\n",
        "# Data Flow: Feedback data flows from the output nodes back to the central system, similar to the initial data flow from input nodes (using secure protocols and structured formats).\n",
        "# Learning from Feedback: The central system incorporates this feedback into its learning process to refine the models and improve future recommendations. This could involve techniques like reinforcement learning or active learning. New data generated by the output nodes' actions can also be used as a source of learning."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d4d29bc"
      },
      "source": [
        "## Choose appropriate technologies\n",
        "\n",
        "### Subtask:\n",
        "Select the technologies for each component, considering aspects like distributed computing, data processing, machine learning frameworks, and communication protocols.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd8c3eef"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify suitable technologies for each component and communication protocols, and document the choices. This addresses steps 1-6 of the subtask by selecting technologies for input, central, and output nodes, choosing communication protocols, considering feedback mechanisms, and documenting these decisions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a75c05b"
      },
      "source": [
        "# 1. Input Nodes:\n",
        "# Technologies:\n",
        "# - Data Collection: Domain-specific sensors, APIs, data connectors.\n",
        "# - Preprocessing: Python with libraries like Pandas, NumPy for data manipulation; potentially edge computing platforms (e.g., AWS IoT Greengrass, Azure IoT Edge) for local processing.\n",
        "# - Transmission: Lightweight clients built with Python or other suitable languages, utilizing chosen communication protocols.\n",
        "\n",
        "# 2. Central Learning/Recommendation System:\n",
        "# Technologies:\n",
        "# - Distributed Computing: Apache Spark, Dask for large-scale data processing and model training.\n",
        "# - Data Processing/Storage: Distributed file systems (e.g., HDFS), cloud storage (e.g., S3, Azure Blob Storage), distributed databases (e.g., Cassandra, MongoDB), data warehouses (e.g., Snowflake, BigQuery).\n",
        "# - Machine Learning Frameworks: TensorFlow, PyTorch, JAX for building and training complex models. Scikit-learn for traditional ML algorithms.\n",
        "# - Model Serving: TensorFlow Serving, TorchServe, FastAPI/Flask for deploying models and serving recommendations.\n",
        "# - Orchestration: Kubernetes, Docker Swarm for managing distributed components.\n",
        "# - Experiment Tracking/MLOps: MLflow, Weights & Biases for managing experiments, tracking metrics, and model versioning.\n",
        "\n",
        "# 3. Output Nodes:\n",
        "# Technologies:\n",
        "# - Receiving Recommendations: Clients built with appropriate languages based on the output node environment (e.g., mobile app development frameworks, web development frameworks, embedded system programming).\n",
        "# - Acting on Recommendations: Integration with actuators, user interfaces, or other systems to implement the recommendations.\n",
        "# - Providing Feedback: APIs or messaging clients to send feedback data back to the central system.\n",
        "\n",
        "# 4. Communication Protocols:\n",
        "# - Data Ingestion (Input to Central):\n",
        "#   - High-throughput, low-latency: gRPC, Apache Kafka.\n",
        "#   - Request/Reply: RESTful APIs (HTTPS).\n",
        "# - Recommendation Delivery (Central to Output):\n",
        "#   - Real-time/Push: WebSockets, Server-Sent Events (SSE), MQTT (for IoT scenarios).\n",
        "#   - Request/Reply: RESTful APIs.\n",
        "# - Feedback Loop (Output to Central):\n",
        "#   - Similar to data ingestion: RESTful APIs, gRPC, message queues.\n",
        "\n",
        "# 5. Feedback Loop Mechanism:\n",
        "# Technologies:\n",
        "# - Data Collection: Event tracking libraries, user interface logging, sensor data.\n",
        "# - Transmission: Chosen communication protocols (e.g., REST, gRPC, message queues).\n",
        "# - Processing in Central System: Dedicated data pipelines to process feedback, potentially using stream processing frameworks (e.g., Spark Streaming, Flink) for real-time feedback incorporation.\n",
        "# - Learning Integration: Techniques like reinforcement learning, online learning, or retraining pipelines to update models based on feedback.\n",
        "\n",
        "# 6. Documentation of Chosen Technologies and Rationale:\n",
        "# - Input Nodes: Chosen for efficient data collection and local preprocessing capabilities.\n",
        "# - Central System: Selected to handle large-scale distributed data processing, advanced machine learning model training, and scalable model serving. Orchestration technologies ensure manageability. MLOps tools support the development lifecycle.\n",
        "# - Output Nodes: Technologies are dependent on the specific application but focus on reliable reception and implementation of recommendations, and efficient feedback transmission.\n",
        "# - Communication Protocols: Selected based on the required communication patterns (streaming vs. request/reply), latency requirements, and reliability needs for different data flows.\n",
        "# - Feedback Loop: Technologies support capturing diverse feedback types and integrating them into the learning process efficiently, potentially in real-time."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62afa0f0"
      },
      "source": [
        "## Implement data ingestion and processing\n",
        "\n",
        "### Subtask:\n",
        "Write code to handle data coming from the input nodes, preprocess it as needed, and prepare it for the central learning system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5fe3698"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a class to simulate receiving and preprocessing data from an input node, including basic data validation, cleaning, and transformation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be2aae61",
        "outputId": "31d9f0bc-dfed-4678-aa01-d07350edab39"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class InputNodeDataHandler:\n",
        "    def __init__(self, expected_columns, column_dtypes, fill_missing_value=None):\n",
        "        \"\"\"\n",
        "        Initializes the data handler with expected data structure and cleaning rules.\n",
        "\n",
        "        Args:\n",
        "            expected_columns (list): List of expected column names.\n",
        "            column_dtypes (dict): Dictionary mapping column names to expected data types.\n",
        "            fill_missing_value (any, optional): Value to fill missing data with. Defaults to None (drop rows with missing values).\n",
        "        \"\"\"\n",
        "        self.expected_columns = expected_columns\n",
        "        self.column_dtypes = column_dtypes\n",
        "        self.fill_missing_value = fill_missing_value\n",
        "\n",
        "    def receive_and_preprocess(self, raw_data):\n",
        "        \"\"\"\n",
        "        Simulates receiving raw data, performs validation and preprocessing.\n",
        "\n",
        "        Args:\n",
        "            raw_data (list of dict): A list of dictionaries representing incoming data records.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A pandas DataFrame containing the processed data,\n",
        "                          or None if preprocessing fails critical validation.\n",
        "        \"\"\"\n",
        "        if not raw_data:\n",
        "            print(\"Received empty data.\")\n",
        "            return None\n",
        "\n",
        "        # Simulate receiving data by creating a DataFrame\n",
        "        try:\n",
        "            df = pd.DataFrame(raw_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating DataFrame: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Data Validation: Check for expected columns\n",
        "        if not all(col in df.columns for col in self.expected_columns):\n",
        "            missing_cols = [col for col in self.expected_columns if col not in df.columns]\n",
        "            print(f\"Missing expected columns: {missing_cols}\")\n",
        "            return None\n",
        "\n",
        "        # Data Validation: Check and enforce data types\n",
        "        for col, dtype in self.column_dtypes.items():\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error converting column '{col}' to {dtype}: {e}\")\n",
        "                    return None # Critical error, stop processing this batch\n",
        "\n",
        "        # Data Cleaning: Handle missing values\n",
        "        if df.isnull().sum().sum() > 0:\n",
        "            if self.fill_missing_value is not None:\n",
        "                df.fillna(self.fill_missing_value, inplace=True)\n",
        "                print(f\"Filled missing values with: {self.fill_missing_value}\")\n",
        "            else:\n",
        "                initial_rows = len(df)\n",
        "                df.dropna(inplace=True)\n",
        "                rows_dropped = initial_rows - len(df)\n",
        "                if rows_dropped > 0:\n",
        "                    print(f\"Dropped {rows_dropped} rows with missing values.\")\n",
        "\n",
        "        # Basic Transformation: Example - simple scaling for a numeric column if exists\n",
        "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "        for col in numeric_cols:\n",
        "             if col in self.expected_columns: # Only transform expected numeric columns\n",
        "                # Avoid division by zero if column is all zeros\n",
        "                if df[col].max() - df[col].min() > 0:\n",
        "                    df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
        "                    print(f\"Scaled column: {col}\")\n",
        "\n",
        "\n",
        "        # Data is now processed and ready for the central system\n",
        "        return df\n",
        "\n",
        "# Example Usage:\n",
        "# Define expected data structure and cleaning rules for a dummy input node\n",
        "expected_cols = ['feature1', 'feature2', 'category', 'timestamp']\n",
        "col_types = {'feature1': 'float32', 'feature2': 'float32', 'category': 'object', 'timestamp': 'datetime64[ns]'}\n",
        "handler = InputNodeDataHandler(expected_cols, col_types, fill_missing_value=0.0)\n",
        "\n",
        "# Simulate receiving some raw data\n",
        "raw_data_example = [\n",
        "    {'feature1': 10.5, 'feature2': 20.1, 'category': 'A', 'timestamp': '2023-01-01T10:00:00'},\n",
        "    {'feature1': 12.0, 'feature2': None, 'category': 'B', 'timestamp': '2023-01-01T10:05:00'},\n",
        "    {'feature1': 11.2, 'feature2': 21.5, 'category': 'A', 'timestamp': '2023-01-01T10:10:00'},\n",
        "    {'feature1': 'invalid', 'feature2': 22.0, 'category': 'C', 'timestamp': '2023-01-01T10:15:00'}, # Invalid data type\n",
        "    {'feature1': 13.5, 'feature2': 23.1, 'category': 'B', 'timestamp': '2023-01-01T10:20:00'},\n",
        "]\n",
        "\n",
        "# Process the data\n",
        "processed_df = handler.receive_and_preprocess(raw_data_example)\n",
        "\n",
        "# Display the processed data\n",
        "if processed_df is not None:\n",
        "    display(processed_df)\n",
        "\n",
        "# Simulate receiving data with missing columns\n",
        "raw_data_missing_col = [\n",
        "     {'feature1': 10.5, 'feature2': 20.1, 'timestamp': '2023-01-01T10:00:00'},\n",
        "]\n",
        "processed_df_missing = handler.receive_and_preprocess(raw_data_missing_col)\n",
        "\n",
        "# Simulate receiving empty data\n",
        "raw_data_empty = []\n",
        "processed_df_empty = handler.receive_and_preprocess(raw_data_empty)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error converting column 'feature1' to float32: could not convert string to float: 'invalid'\n",
            "Missing expected columns: ['category']\n",
            "Received empty data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e80db2a"
      },
      "source": [
        "## Develop the central learning system\n",
        "\n",
        "### Subtask:\n",
        "Build the core of the system, which will likely involve a machine learning model that learns from the input data and generates recommendations. This might involve choosing a specific model architecture and training procedure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9214ba55"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a class for the central learning system that will encapsulate the machine learning model and its training logic, choose and instantiate a suitable machine learning model, implement a basic training procedure, and add methods for saving and loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d4e7d4b6",
        "outputId": "f9636303-6053-471a-960a-87628fd8734f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "class CentralLearningSystem:\n",
        "    def __init__(self, input_shape, num_recommendation_outputs):\n",
        "        \"\"\"\n",
        "        Initializes the Central Learning System with a neural network model.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): The shape of the input data (excluding batch size).\n",
        "            num_recommendation_outputs (int): The number of outputs for recommendations.\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.num_recommendation_outputs = num_recommendation_outputs\n",
        "        self.model = self._build_model()\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        self.loss = keras.losses.MeanSquaredError() # Example loss for regression-like recommendations\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Builds a simple neural network model.\"\"\"\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        x = layers.Dense(64, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(self.num_recommendation_outputs)(x) # Linear output for regression\n",
        "        model = keras.Model(inputs, outputs, name=\"central_recommendation_model\")\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        \"\"\"Compiles the built model.\"\"\"\n",
        "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
        "\n",
        "    def train(self, data, labels, epochs=1, batch_size=32):\n",
        "        \"\"\"\n",
        "        Trains the model on the provided data.\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray or tf.data.Dataset): Training data.\n",
        "            labels (np.ndarray or tf.data.Dataset): Training labels/targets.\n",
        "            epochs (int): Number of epochs to train for.\n",
        "            batch_size (int): Batch size for training.\n",
        "        \"\"\"\n",
        "        print(\"Starting model training...\")\n",
        "        history = self.model.fit(data, labels, epochs=epochs, batch_size=batch_size)\n",
        "        print(\"Training finished.\")\n",
        "        return history\n",
        "\n",
        "    def update_model(self, new_data, new_labels, epochs=1, batch_size=32):\n",
        "        \"\"\"\n",
        "        Updates the model incrementally with new data.\n",
        "\n",
        "        Args:\n",
        "            new_data (np.ndarray or tf.data.Dataset): New training data.\n",
        "            new_labels (np.ndarray or tf.data.Dataset): New training labels/targets.\n",
        "            epochs (int): Number of epochs for incremental training.\n",
        "            batch_size (int): Batch size for incremental training.\n",
        "        \"\"\"\n",
        "        print(\"Updating model with new data...\")\n",
        "        history = self.model.fit(new_data, new_labels, epochs=epochs, batch_size=batch_size)\n",
        "        print(\"Model update finished.\")\n",
        "        return history\n",
        "\n",
        "\n",
        "    def generate_recommendations(self, input_data):\n",
        "        \"\"\"\n",
        "        Generates recommendations for the given input data.\n",
        "\n",
        "        Args:\n",
        "            input_data (np.ndarray or tf.data.Dataset): Data to generate recommendations for.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Generated recommendations.\n",
        "        \"\"\"\n",
        "        print(\"Generating recommendations...\")\n",
        "        predictions = self.model.predict(input_data)\n",
        "        return predictions\n",
        "\n",
        "    def save_model(self, filepath=\"central_model.keras\"):\n",
        "        \"\"\"\n",
        "        Saves the trained model to a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the model file.\n",
        "        \"\"\"\n",
        "        print(f\"Saving model to {filepath}...\")\n",
        "        self.model.save(filepath)\n",
        "        print(\"Model saved.\")\n",
        "\n",
        "    def load_model(self, filepath=\"central_model.keras\"):\n",
        "        \"\"\"\n",
        "        Loads a trained model from a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the model file.\n",
        "        \"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"Loading model from {filepath}...\")\n",
        "            self.model = keras.models.load_model(filepath)\n",
        "            print(\"Model loaded.\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {filepath}. Cannot load.\")\n",
        "\n",
        "\n",
        "# Example Usage:\n",
        "# Assume input data is 10-dimensional vectors and we want 5 recommendation outputs\n",
        "input_dim = 10\n",
        "recommendation_outputs = 5\n",
        "cls = CentralLearningSystem(input_shape=(input_dim,), num_recommendation_outputs=recommendation_outputs)\n",
        "\n",
        "# Compile the model\n",
        "cls.compile_model()\n",
        "cls.model.summary()\n",
        "\n",
        "# Generate some dummy data for training\n",
        "dummy_data = np.random.rand(100, input_dim).astype(np.float32)\n",
        "dummy_labels = np.random.rand(100, recommendation_outputs).astype(np.float32) # Dummy regression targets\n",
        "\n",
        "# Train the model\n",
        "cls.train(dummy_data, dummy_labels, epochs=5)\n",
        "\n",
        "# Simulate new data arrival and update the model\n",
        "new_dummy_data = np.random.rand(20, input_dim).astype(np.float32)\n",
        "new_dummy_labels = np.random.rand(20, recommendation_outputs).astype(np.float32)\n",
        "cls.update_model(new_dummy_data, new_dummy_labels, epochs=2)\n",
        "\n",
        "# Generate recommendations for some new data\n",
        "new_input_for_recommendation = np.random.rand(5, input_dim).astype(np.float32)\n",
        "recommendations = cls.generate_recommendations(new_input_for_recommendation)\n",
        "print(\"\\nGenerated Recommendations:\")\n",
        "print(recommendations)\n",
        "\n",
        "# Save the model\n",
        "cls.save_model(\"my_central_model.keras\")\n",
        "\n",
        "# Create a new instance and load the model\n",
        "new_cls = CentralLearningSystem(input_shape=(input_dim,), num_recommendation_outputs=recommendation_outputs)\n",
        "new_cls.load_model(\"my_central_model.keras\")\n",
        "\n",
        "# Generate recommendations with the loaded model\n",
        "recommendations_loaded = new_cls.generate_recommendations(new_input_for_recommendation)\n",
        "print(\"\\nGenerated Recommendations with Loaded Model:\")\n",
        "print(recommendations_loaded)\n",
        "\n",
        "# Clean up the saved model file\n",
        "# os.remove(\"my_central_model.keras\") # Uncomment to remove the file after testing"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"central_recommendation_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"central_recommendation_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,989\u001b[0m (39.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,989</span> (39.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,989\u001b[0m (39.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,989</span> (39.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 486ms/step - loss: 0.4253\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2489\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1706 \n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1186 \n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1135\n",
            "Training finished.\n",
            "Updating model with new data...\n",
            "Epoch 1/2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0946\n",
            "Epoch 2/2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 0.0968\n",
            "Model update finished.\n",
            "Generating recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
            "\n",
            "Generated Recommendations:\n",
            "[[0.58363295 0.4795627  0.48926106 0.3982363  0.4785596 ]\n",
            " [0.4531225  0.592127   0.67223656 0.46496353 0.5750772 ]\n",
            " [0.4323017  0.41731906 0.52491856 0.37161222 0.41320324]\n",
            " [0.43924284 0.49161872 0.44198418 0.43259922 0.43848175]\n",
            " [0.5882393  0.535986   0.64066577 0.7022638  0.6318971 ]]\n",
            "Saving model to my_central_model.keras...\n",
            "Model saved.\n",
            "Loading model from my_central_model.keras...\n",
            "Model loaded.\n",
            "Generating recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "Generated Recommendations with Loaded Model:\n",
            "[[0.58363295 0.4795627  0.48926106 0.3982363  0.4785596 ]\n",
            " [0.4531225  0.592127   0.67223656 0.46496353 0.5750772 ]\n",
            " [0.4323017  0.41731906 0.52491856 0.37161222 0.41320324]\n",
            " [0.43924284 0.49161872 0.44198418 0.43259922 0.43848175]\n",
            " [0.5882393  0.535986   0.64066577 0.7022638  0.6318971 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52e6610"
      },
      "source": [
        "## Implement the recommendation engine\n",
        "\n",
        "### Subtask:\n",
        "Develop the logic for generating recommendations based on the learned model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25226307"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the post-processing logic for generating recommendations based on the raw model output, including handling edge cases, and integrate this logic into the `generate_recommendations` method of the `CentralLearningSystem` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d1885ae",
        "outputId": "a849dcf8-508e-4d06-aac4-3a7938cdadc2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class CentralLearningSystem:\n",
        "    def __init__(self, input_shape, num_recommendation_outputs):\n",
        "        \"\"\"\n",
        "        Initializes the Central Learning System with a neural network model.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): The shape of the input data (excluding batch size).\n",
        "            num_recommendation_outputs (int): The number of outputs for recommendations.\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.num_recommendation_outputs = num_recommendation_outputs\n",
        "        self.model = self._build_model()\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        self.loss = keras.losses.MeanSquaredError() # Example loss for regression-like recommendations\n",
        "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Builds a simple neural network model.\"\"\"\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        x = layers.Dense(64, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(self.num_recommendation_outputs)(x) # Linear output for regression\n",
        "        model = keras.Model(inputs, outputs, name=\"central_recommendation_model\")\n",
        "        return model\n",
        "\n",
        "    def train(self, data, labels, epochs=1, batch_size=32):\n",
        "        \"\"\"\n",
        "        Trains the model on the provided data.\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray or tf.data.Dataset): Training data.\n",
        "            labels (np.ndarray or tf.data.Dataset): Training labels/targets.\n",
        "            epochs (int): Number of epochs to train for.\n",
        "            batch_size (int): Batch size for training.\n",
        "        \"\"\"\n",
        "        print(\"Starting model training...\")\n",
        "        history = self.model.fit(data, labels, epochs=epochs, batch_size=batch_size)\n",
        "        print(\"Training finished.\")\n",
        "        return history\n",
        "\n",
        "    def update_model(self, new_data, new_labels, epochs=1, batch_size=32):\n",
        "        \"\"\"\n",
        "        Updates the model incrementally with new data.\n",
        "\n",
        "        Args:\n",
        "            new_data (np.ndarray or tf.data.Dataset): New training data.\n",
        "            new_labels (np.ndarray or tf.data.Dataset): New training labels/targets.\n",
        "            epochs (int): Number of epochs for incremental training.\n",
        "            batch_size (int): Batch size for incremental training.\n",
        "        \"\"\"\n",
        "        print(\"Updating model with new data...\")\n",
        "        history = self.model.fit(new_data, new_labels, epochs=epochs, batch_size=batch_size)\n",
        "        print(\"Model update finished.\")\n",
        "        return history\n",
        "\n",
        "    def _postprocess_recommendations(self, raw_predictions):\n",
        "        \"\"\"\n",
        "        Post-processes raw model predictions into final recommendations.\n",
        "\n",
        "        Args:\n",
        "            raw_predictions (np.ndarray): Raw output from the model.\n",
        "\n",
        "        Returns:\n",
        "            list or np.ndarray: Formatted and processed recommendations.\n",
        "        \"\"\"\n",
        "        print(\"Post-processing raw predictions...\")\n",
        "        # Example post-processing: If predictions are scores, rank them and return top-k\n",
        "        # Assuming higher scores are better recommendations\n",
        "        # For this example, let's rank the recommendations for each input sample\n",
        "        processed_recommendations = []\n",
        "        for single_prediction in raw_predictions:\n",
        "            # Get the indices that would sort the prediction in descending order\n",
        "            ranked_indices = np.argsort(single_prediction)[::-1]\n",
        "            # For simplicity, let's return the ranked indices as recommendations\n",
        "            # In a real scenario, these indices would map to actual recommendation items\n",
        "            processed_recommendations.append(ranked_indices.tolist())\n",
        "\n",
        "        print(\"Post-processing finished.\")\n",
        "        return processed_recommendations\n",
        "\n",
        "\n",
        "    def generate_recommendations(self, input_data):\n",
        "        \"\"\"\n",
        "        Generates and post-processes recommendations for the given input data.\n",
        "\n",
        "        Args:\n",
        "            input_data (np.ndarray or tf.data.Dataset): Data to generate recommendations for.\n",
        "\n",
        "        Returns:\n",
        "            list or np.ndarray: Formatted and processed recommendations.\n",
        "        \"\"\"\n",
        "        print(\"Generating raw recommendations...\")\n",
        "        raw_predictions = self.model.predict(input_data)\n",
        "        recommendations = self._postprocess_recommendations(raw_predictions)\n",
        "        return recommendations\n",
        "\n",
        "    def save_model(self, filepath=\"central_model.keras\"):\n",
        "        \"\"\"\n",
        "        Saves the trained model to a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the model file.\n",
        "        \"\"\"\n",
        "        print(f\"Saving model to {filepath}...\")\n",
        "        self.model.save(filepath)\n",
        "        print(\"Model saved.\")\n",
        "\n",
        "    def load_model(self, filepath=\"central_model.keras\"):\n",
        "        \"\"\"\n",
        "        Loads a trained model from a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the model file.\n",
        "        \"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"Loading model from {filepath}...\")\n",
        "            self.model = keras.models.load_model(filepath)\n",
        "            print(\"Model loaded.\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {filepath}. Cannot load.\")\n",
        "\n",
        "\n",
        "# Example Usage:\n",
        "# Assume input data is 10-dimensional vectors and we want 5 recommendation outputs\n",
        "input_dim = 10\n",
        "recommendation_outputs = 5\n",
        "cls = CentralLearningSystem(input_shape=(input_dim,), num_recommendation_outputs=recommendation_outputs)\n",
        "\n",
        "\n",
        "# Generate some dummy data for training\n",
        "dummy_data = np.random.rand(100, input_dim).astype(np.float32)\n",
        "dummy_labels = np.random.rand(100, recommendation_outputs).astype(np.float32) # Dummy regression targets\n",
        "\n",
        "# Train the model\n",
        "cls.train(dummy_data, dummy_labels, epochs=5)\n",
        "\n",
        "# Generate recommendations for some new data\n",
        "new_input_for_recommendation = np.random.rand(5, input_dim).astype(np.float32)\n",
        "recommendations = cls.generate_recommendations(new_input_for_recommendation)\n",
        "print(\"\\nGenerated and Post-processed Recommendations:\")\n",
        "print(recommendations)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - loss: 0.3197\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2152 \n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1524\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1150 \n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0957 \n",
            "Training finished.\n",
            "Generating raw recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "\n",
            "Generated and Post-processed Recommendations:\n",
            "[[2, 3, 1, 4, 0], [3, 4, 0, 2, 1], [2, 3, 4, 1, 0], [0, 4, 1, 3, 2], [3, 4, 0, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b132ff2"
      },
      "source": [
        "## Design and implement the output mechanism\n",
        "\n",
        "### Subtask:\n",
        "Design and implement the system for delivering recommendations to the output nodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b5c9184"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a class `RecommendationOutput` to encapsulate the logic for sending recommendations, include a method `send_recommendations` to simulate sending recommendations to target output nodes, and provide a basic usage example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057d8d58",
        "outputId": "55112c42-6ea6-499e-c833-b00dfc471dfb"
      },
      "source": [
        "class RecommendationOutput:\n",
        "    def __init__(self, output_node_id):\n",
        "        \"\"\"\n",
        "        Initializes a RecommendationOutput instance for a specific output node.\n",
        "\n",
        "        Args:\n",
        "            output_node_id (str): A unique identifier for the target output node.\n",
        "        \"\"\"\n",
        "        self.output_node_id = output_node_id\n",
        "        print(f\"RecommendationOutput initialized for node: {self.output_node_id}\")\n",
        "\n",
        "    def send_recommendations(self, recommendations, format_type=\"json\"):\n",
        "        \"\"\"\n",
        "        Simulates sending processed recommendations to the target output node.\n",
        "\n",
        "        Args:\n",
        "            recommendations (list or np.ndarray): The processed recommendations to send.\n",
        "            format_type (str): The desired format for sending recommendations (e.g., \"json\", \"csv\").\n",
        "                               Defaults to \"json\".\n",
        "        \"\"\"\n",
        "        print(f\"Attempting to send recommendations to node: {self.output_node_id}\")\n",
        "        print(f\"Format type: {format_type}\")\n",
        "\n",
        "        # Simulate the transmission mechanism\n",
        "        if format_type == \"json\":\n",
        "            # In a real system, this would involve serializing to JSON and sending\n",
        "            print(f\"Simulating sending JSON recommendations to {self.output_node_id}:\")\n",
        "            # Convert numpy arrays/lists to a serializable format for simulation\n",
        "            if isinstance(recommendations, np.ndarray):\n",
        "                 print(recommendations.tolist())\n",
        "            else:\n",
        "                 print(recommendations)\n",
        "        elif format_type == \"csv\":\n",
        "            # In a real system, this would involve formatting as CSV and sending\n",
        "            print(f\"Simulating sending CSV recommendations to {self.output_node_id}:\")\n",
        "            # For simplicity, just print a representation\n",
        "            if isinstance(recommendations, np.ndarray):\n",
        "                 print(\"CSV representation (dummy):\")\n",
        "                 for rec in recommendations:\n",
        "                     print(\",\".join(map(str, rec)))\n",
        "            else:\n",
        "                 print(\"CSV representation (dummy):\")\n",
        "                 for rec in recommendations:\n",
        "                     print(\",\".join(map(str, rec)))\n",
        "        else:\n",
        "            print(f\"Unsupported format type: {format_type}. Sending raw data.\")\n",
        "            print(recommendations)\n",
        "\n",
        "        print(f\"Simulation of sending recommendations to {self.output_node_id} finished.\")\n",
        "\n",
        "\n",
        "# Basic Example Usage:\n",
        "# Create a dummy output node handler\n",
        "dummy_output_node_id = \"output_node_123\"\n",
        "output_handler = RecommendationOutput(dummy_output_node_id)\n",
        "\n",
        "# Example processed recommendations (e.g., from the CentralLearningSystem)\n",
        "example_recommendations = [[1, 3, 0], [4, 2, 1]] # Example output from ranking\n",
        "\n",
        "# Send the recommendations using the defined method\n",
        "output_handler.send_recommendations(example_recommendations, format_type=\"json\")\n",
        "\n",
        "# Send recommendations in a different format\n",
        "output_handler.send_recommendations(example_recommendations, format_type=\"csv\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecommendationOutput initialized for node: output_node_123\n",
            "Attempting to send recommendations to node: output_node_123\n",
            "Format type: json\n",
            "Simulating sending JSON recommendations to output_node_123:\n",
            "[[1, 3, 0], [4, 2, 1]]\n",
            "Simulation of sending recommendations to output_node_123 finished.\n",
            "Attempting to send recommendations to node: output_node_123\n",
            "Format type: csv\n",
            "Simulating sending CSV recommendations to output_node_123:\n",
            "CSV representation (dummy):\n",
            "1,3,0\n",
            "4,2,1\n",
            "Simulation of sending recommendations to output_node_123 finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1fa7caf"
      },
      "source": [
        "## Establish communication between components\n",
        "\n",
        "### Subtask:\n",
        "Set up the necessary communication channels and protocols for seamless interaction between all parts of the architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5e7b4f3"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the communication patterns, protocols, data formats, and security considerations for data ingestion (input to central), recommendation delivery (central to output), and the feedback loop (output to central), and document this setup. This covers steps 1 through 7 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19e932c6",
        "outputId": "c9d35ae2-da2d-4a39-a6f2-0045fa825d76"
      },
      "source": [
        "# Documentation of Communication Setup\n",
        "\n",
        "# 1. Data Ingestion (Input Nodes to Central System)\n",
        "# Communication Pattern: Asynchronous, decoupled data streaming.\n",
        "# Protocol: Apache Kafka (TCP/IP based) - Chosen for high throughput, fault tolerance, and scalability.\n",
        "# Data Format: Protobuf - Provides efficient serialization and deserialization, and clear schema definition.\n",
        "# Security Considerations:\n",
        "# - Encryption: TLS/SSL for data in transit.\n",
        "# - Authentication: SASL (e.g., SCRAM-SHA-512) for client authentication.\n",
        "# - Authorization: Access Control Lists (ACLs) on Kafka topics to control which input nodes can write.\n",
        "\n",
        "# 2. Recommendation Delivery (Central System to Output Nodes)\n",
        "# Communication Pattern: Pull mechanism (Output nodes request recommendations).\n",
        "# Protocol: HTTP/2 or gRPC over HTTPS - Chosen for efficient request/response and potential for multiplexing (HTTP/2) or structured communication (gRPC). HTTPS ensures encryption in transit.\n",
        "# Data Format: JSON - Widely supported and easy to parse by various output node environments.\n",
        "# Security Considerations:\n",
        "# - Encryption: HTTPS/TLS for data in transit.\n",
        "# - Authentication: API keys or token-based authentication (e.g., OAuth 2.0) for output nodes to access the recommendation endpoint.\n",
        "# - Authorization: Role-based access control (RBAC) on the central system's API to ensure output nodes only receive relevant recommendations.\n",
        "\n",
        "# 3. Feedback Loop (Output Nodes to Central System)\n",
        "# Communication Pattern: Asynchronous event-driven (Output nodes send feedback events).\n",
        "# Protocol: Apache Kafka (TCP/IP based) or MQTT (for lightweight IoT output nodes). Chosen for decoupling and handling potentially high volume of feedback.\n",
        "# Data Format: Protobuf or JSON - Depending on the complexity and volume of feedback data. Protobuf for structured, high-volume feedback; JSON for simpler feedback structures.\n",
        "# Security Considerations:\n",
        "# - Encryption: TLS/SSL for data in transit.\n",
        "# - Authentication: SASL (Kafka) or username/password/certificates (MQTT) for output node authentication.\n",
        "# - Authorization: ACLs on Kafka/MQTT topics to control which output nodes can publish feedback.\n",
        "\n",
        "# Summary Table:\n",
        "# | Flow                     | Pattern          | Protocol(s)           | Data Format | Security Considerations                                  |\n",
        "# |--------------------------|------------------|-----------------------|-------------|----------------------------------------------------------|\n",
        "# | Input -> Central         | Asynchronous     | Kafka (TCP/IP)        | Protobuf    | TLS/SSL, SASL, Kafka ACLs                                |\n",
        "# | Central -> Output        | Pull             | HTTP/2 or gRPC (HTTPS)| JSON        | HTTPS/TLS, API Keys/Tokens, RBAC                         |\n",
        "# | Output -> Central        | Asynchronous     | Kafka or MQTT (TCP/IP)| Protobuf/JSON| TLS/SSL, SASL/MQTT Auth, Kafka/MQTT ACLs                 |\n",
        "\n",
        "print(\"Communication setup documented.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Communication setup documented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894ed3d9"
      },
      "source": [
        "## Implement feedback loops\n",
        "\n",
        "### Subtask:\n",
        "Incorporate mechanisms for the output nodes to provide feedback on the recommendations, which can be used to further refine the central learning system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2289c1f"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the feedback mechanism in the `RecommendationOutput` and `CentralLearningSystem` classes, including a method to send feedback from the output node and methods in the central system to receive and process this feedback for model updates. Update the `update_model` method to incorporate feedback processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "091b2758",
        "outputId": "d224f239-7a5f-48e7-dea3-38128cabb10c"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "class RecommendationOutput:\n",
        "    def __init__(self, output_node_id):\n",
        "        \"\"\"\n",
        "        Initializes a RecommendationOutput instance for a specific output node.\n",
        "\n",
        "        Args:\n",
        "            output_node_id (str): A unique identifier for the target output node.\n",
        "        \"\"\"\n",
        "        self.output_node_id = output_node_id\n",
        "        print(f\"RecommendationOutput initialized for node: {self.output_node_id}\")\n",
        "\n",
        "    def send_recommendations(self, recommendations, format_type=\"json\"):\n",
        "        \"\"\"\n",
        "        Simulates sending processed recommendations to the target output node.\n",
        "\n",
        "        Args:\n",
        "            recommendations (list or np.ndarray): The processed recommendations to send.\n",
        "            format_type (str): The desired format for sending recommendations (e.g., \"json\", \"csv\").\n",
        "                               Defaults to \"json\".\n",
        "        \"\"\"\n",
        "        print(f\"Attempting to send recommendations to node: {self.output_node_id}\")\n",
        "        print(f\"Format type: {format_type}\")\n",
        "\n",
        "        # Simulate the transmission mechanism\n",
        "        if format_type == \"json\":\n",
        "            # In a real system, this would involve serializing to JSON and sending\n",
        "            print(f\"Simulating sending JSON recommendations to {self.output_node_id}:\")\n",
        "            # Convert numpy arrays/lists to a serializable format for simulation\n",
        "            if isinstance(recommendations, np.ndarray):\n",
        "                 print(recommendations.tolist())\n",
        "            else:\n",
        "                 print(recommendations)\n",
        "        elif format_type == \"csv\":\n",
        "            # In a real system, this would involve formatting as CSV and sending\n",
        "            print(f\"Simulating sending CSV recommendations to {self.output_node_id}:\")\n",
        "            # For simplicity, just print a representation\n",
        "            if isinstance(recommendations, np.ndarray):\n",
        "                 print(\"CSV representation (dummy):\")\n",
        "                 for rec in recommendations:\n",
        "                     print(\",\".join(map(str, rec)))\n",
        "            else:\n",
        "                 print(\"CSV representation (dummy):\")\n",
        "                 for rec in recommendations:\n",
        "                     print(\",\".join(map(str, rec)))\n",
        "        else:\n",
        "            print(f\"Unsupported format type: {format_type}. Sending raw data.\")\n",
        "            print(recommendations)\n",
        "\n",
        "        print(f\"Simulation of sending recommendations to {self.output_node_id} finished.\")\n",
        "\n",
        "    def send_feedback(self, feedback_data):\n",
        "        \"\"\"\n",
        "        Simulates sending feedback from the output node back to the central system.\n",
        "\n",
        "        Args:\n",
        "            feedback_data (dict): A dictionary containing feedback information.\n",
        "                                  Example format: {'recommendation_id': 'rec_123', 'rating': 4, 'action': 'accepted'}\n",
        "        \"\"\"\n",
        "        print(f\"Attempting to send feedback from node: {self.output_node_id}\")\n",
        "        print(f\"Feedback data: {feedback_data}\")\n",
        "        # In a real system, this would involve serializing the feedback data and sending it\n",
        "        # to the central system's feedback ingestion endpoint (e.g., via Kafka or REST).\n",
        "        print(f\"Simulation of sending feedback from {self.output_node_id} finished.\")\n",
        "        # Return the feedback data for the central system to process in this simulation\n",
        "        return feedback_data\n",
        "\n",
        "\n",
        "class CentralLearningSystem:\n",
        "    def __init__(self, input_shape, num_recommendation_outputs):\n",
        "        \"\"\"\n",
        "        Initializes the Central Learning System with a neural network model.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): The shape of the input data (excluding batch size).\n",
        "            num_recommendation_outputs (int): The number of outputs for recommendations.\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.num_recommendation_outputs = num_recommendation_outputs\n",
        "        self.model = self._build_model()\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        self.loss = keras.losses.MeanSquaredError() # Example loss for regression-like recommendations\n",
        "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
        "        self.feedback_buffer = [] # Buffer to store incoming feedback\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Builds a simple neural network model.\"\"\"\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        x = layers.Dense(64, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(self.num_recommendation_outputs)(x) # Linear output for regression\n",
        "        model = keras.Model(inputs, outputs, name=\"central_recommendation_model\")\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        \"\"\"Compiles the built model.\"\"\"\n",
        "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
        "\n",
        "    def train(self, data, labels, epochs=1, batch_size=32):\n",
        "        \"\"\"\n",
        "        Trains the model on the provided data.\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray or tf.data.Dataset): Training data.\n",
        "            labels (np.ndarray or tf.data.Dataset): Training labels/targets.\n",
        "            epochs (int): Number of epochs to train for.\n",
        "            batch_size (int): Batch size for training.\n",
        "        \"\"\"\n",
        "        print(\"Starting model training...\")\n",
        "        history = self.model.fit(data, labels, epochs=epochs, batch_size=batch_size)\n",
        "        print(\"Training finished.\")\n",
        "        return history\n",
        "\n",
        "    def receive_feedback(self, feedback_data):\n",
        "        \"\"\"\n",
        "        Receives feedback data from an output node and adds it to the buffer.\n",
        "\n",
        "        Args:\n",
        "            feedback_data (dict): A dictionary containing feedback information.\n",
        "        \"\"\"\n",
        "        print(\"Central system received feedback.\")\n",
        "        self.feedback_buffer.append(feedback_data)\n",
        "        print(f\"Current feedback buffer size: {len(self.feedback_buffer)}\")\n",
        "\n",
        "    def process_feedback(self):\n",
        "        \"\"\"\n",
        "        Processes the accumulated feedback data from the buffer.\n",
        "        This is a placeholder for actual feedback processing logic.\n",
        "\n",
        "        Returns:\n",
        "            list: Processed feedback data.\n",
        "        \"\"\"\n",
        "        print(f\"Processing {len(self.feedback_buffer)} feedback entries...\")\n",
        "        processed_feedback = self.feedback_buffer.copy()\n",
        "        self.feedback_buffer = [] # Clear the buffer after processing\n",
        "        print(\"Feedback processing finished. Buffer cleared.\")\n",
        "        return processed_feedback\n",
        "\n",
        "    def update_model_with_feedback(self, epochs=1, batch_size=32):\n",
        "        \"\"\"\n",
        "        Updates the model by incorporating the processed feedback data.\n",
        "        This is a simplified example; real-world scenarios would involve\n",
        "        creating a training dataset from feedback and potentially other data.\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of epochs for incremental training.\n",
        "            batch_size (int): Batch size for incremental training.\n",
        "        \"\"\"\n",
        "        processed_feedback = self.process_feedback()\n",
        "\n",
        "        if not processed_feedback:\n",
        "            print(\"No feedback to update the model with.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Preparing data for model update from feedback...\")\n",
        "        # In a real scenario, you would generate (input, target) pairs from feedback.\n",
        "        # For this simplified example, let's assume feedback provides direct\n",
        "        # input-target pairs for incremental learning. This is highly simplified.\n",
        "        # A more realistic approach would involve:\n",
        "        # 1. Joining feedback with historical data to get input features.\n",
        "        # 2. Defining targets based on feedback (e.g., recommendation acceptance as a positive signal).\n",
        "        # 3. Potentially using techniques like reinforcement learning or re-sampling.\n",
        "\n",
        "        # Dummy implementation: Create dummy data/labels from the number of feedback entries\n",
        "        # This is NOT a realistic way to use feedback for training, but demonstrates the flow.\n",
        "        num_feedback_entries = len(processed_feedback)\n",
        "        if num_feedback_entries > 0:\n",
        "             # Simulate creating input data and target labels from feedback\n",
        "            feedback_input_data = np.random.rand(num_feedback_entries, self.input_shape[0]).astype(np.float32)\n",
        "            feedback_labels = np.random.rand(num_feedback_entries, self.num_recommendation_outputs).astype(np.float32) # Dummy targets\n",
        "\n",
        "            print(f\"Updating model with {num_feedback_entries} feedback entries...\")\n",
        "            history = self.model.fit(feedback_input_data, feedback_labels, epochs=epochs, batch_size=batch_size)\n",
        "            print(\"Model update based on feedback finished.\")\n",
        "            return history\n",
        "        else:\n",
        "            print(\"No valid feedback data to create training samples.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def generate_recommendations(self, input_data):\n",
        "        \"\"\"\n",
        "        Generates and post-processes recommendations for the given input data.\n",
        "\n",
        "        Args:\n",
        "            input_data (np.ndarray or tf.data.Dataset): Data to generate recommendations for.\n",
        "\n",
        "        Returns:\n",
        "            list or np.ndarray: Formatted and processed recommendations.\n",
        "        \"\"\"\n",
        "        print(\"Generating raw recommendations...\")\n",
        "        raw_predictions = self.model.predict(input_data)\n",
        "        recommendations = self._postprocess_recommendations(raw_predictions)\n",
        "        return recommendations\n",
        "\n",
        "    def _postprocess_recommendations(self, raw_predictions):\n",
        "        \"\"\"\n",
        "        Post-processes raw model predictions into final recommendations.\n",
        "\n",
        "        Args:\n",
        "            raw_predictions (np.ndarray): Raw output from the model.\n",
        "\n",
        "        Returns:\n",
        "            list or np.ndarray: Formatted and processed recommendations.\n",
        "        \"\"\"\n",
        "        print(\"Post-processing raw predictions...\")\n",
        "        # Example post-processing: If predictions are scores, rank them and return top-k\n",
        "        # Assuming higher scores are better recommendations\n",
        "        # For this example, let's rank the recommendations for each input sample\n",
        "        processed_recommendations = []\n",
        "        for single_prediction in raw_predictions:\n",
        "            # Get the indices that would sort the prediction in descending order\n",
        "            ranked_indices = np.argsort(single_prediction)[::-1]\n",
        "            # For simplicity, let's return the ranked indices as recommendations\n",
        "            # In a real scenario, these indices would map to actual recommendation items\n",
        "            processed_recommendations.append(ranked_indices.tolist())\n",
        "\n",
        "        print(\"Post-processing finished.\")\n",
        "        return processed_recommendations\n",
        "\n",
        "\n",
        "    def save_model(self, filepath=\"central_model.keras\"):\n",
        "        \"\"\"\n",
        "        Saves the trained model to a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to save the model file.\n",
        "        \"\"\"\n",
        "        print(f\"Saving model to {filepath}...\")\n",
        "        self.model.save(filepath)\n",
        "        print(\"Model saved.\")\n",
        "\n",
        "    def load_model(self, filepath=\"central_model.keras\"):\n",
        "        \"\"\"\n",
        "        Loads a trained model from a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the model file.\n",
        "        \"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"Loading model from {filepath}...\")\n",
        "            self.model = keras.models.load_model(filepath)\n",
        "            print(\"Model loaded.\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {filepath}. Cannot load.\")\n",
        "\n",
        "\n",
        "# Example Usage Demonstrating Feedback Loop:\n",
        "input_dim = 10\n",
        "recommendation_outputs = 5\n",
        "\n",
        "# Initialize Central System\n",
        "cls = CentralLearningSystem(input_shape=(input_dim,), num_recommendation_outputs=recommendation_outputs)\n",
        "cls.compile_model()\n",
        "\n",
        "# Generate some dummy data for initial training\n",
        "dummy_data = np.random.rand(100, input_dim).astype(np.float32)\n",
        "dummy_labels = np.random.rand(100, recommendation_outputs).astype(np.float32)\n",
        "cls.train(dummy_data, dummy_labels, epochs=5)\n",
        "\n",
        "# Simulate an output node receiving recommendations and sending feedback\n",
        "output_node_handler = RecommendationOutput(\"output_node_789\")\n",
        "\n",
        "# Simulate input data for recommendation\n",
        "input_for_recommendation = np.random.rand(1, input_dim).astype(np.float32)\n",
        "recommendations = cls.generate_recommendations(input_for_recommendation)\n",
        "print(\"\\nRecommendations sent to output node:\")\n",
        "print(recommendations)\n",
        "\n",
        "# Simulate the output node providing feedback\n",
        "# Feedback data format: {'recommendation_id': ..., 'action': ...}\n",
        "# In a real system, recommendation_id would link feedback to specific recommendations.\n",
        "# Here, we'll use a dummy ID and action.\n",
        "feedback_data_1 = {'recommendation_id': 'rec_abc', 'action': 'accepted', 'output_node': output_node_handler.output_node_id}\n",
        "feedback_data_2 = {'recommendation_id': 'rec_def', 'action': 'rejected', 'output_node': output_node_handler.output_node_id}\n",
        "\n",
        "# Output node sends feedback\n",
        "sent_feedback_1 = output_node_handler.send_feedback(feedback_data_1)\n",
        "sent_feedback_2 = output_node_handler.send_feedback(feedback_data_2)\n",
        "\n",
        "\n",
        "# Central system receives feedback (simulated by calling receive_feedback)\n",
        "cls.receive_feedback(sent_feedback_1)\n",
        "cls.receive_feedback(sent_feedback_2)\n",
        "\n",
        "# Central system updates model using feedback\n",
        "# Note: The current update_model_with_feedback is a simplified placeholder.\n",
        "cls.update_model_with_feedback(epochs=1)\n",
        "\n",
        "# Generate recommendations again after model update to observe potential changes\n",
        "print(\"\\nGenerating recommendations after feedback-based update:\")\n",
        "recommendations_after_feedback = cls.generate_recommendations(input_for_recommendation)\n",
        "print(recommendations_after_feedback)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - loss: 0.4008\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2453 \n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1475 \n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1059 \n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1097 \n",
            "Training finished.\n",
            "RecommendationOutput initialized for node: output_node_789\n",
            "Generating raw recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "\n",
            "Recommendations sent to output node:\n",
            "[[1, 3, 0, 4, 2]]\n",
            "Attempting to send feedback from node: output_node_789\n",
            "Feedback data: {'recommendation_id': 'rec_abc', 'action': 'accepted', 'output_node': 'output_node_789'}\n",
            "Simulation of sending feedback from output_node_789 finished.\n",
            "Attempting to send feedback from node: output_node_789\n",
            "Feedback data: {'recommendation_id': 'rec_def', 'action': 'rejected', 'output_node': 'output_node_789'}\n",
            "Simulation of sending feedback from output_node_789 finished.\n",
            "Central system received feedback.\n",
            "Current feedback buffer size: 1\n",
            "Central system received feedback.\n",
            "Current feedback buffer size: 2\n",
            "Processing 2 feedback entries...\n",
            "Feedback processing finished. Buffer cleared.\n",
            "Preparing data for model update from feedback...\n",
            "Updating model with 2 feedback entries...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.1719\n",
            "Model update based on feedback finished.\n",
            "\n",
            "Generating recommendations after feedback-based update:\n",
            "Generating raw recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "[[1, 3, 0, 4, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e9b5e7"
      },
      "source": [
        "## Testing and evaluation\n",
        "\n",
        "### Subtask:\n",
        "Test the integrated system to ensure it functions correctly and the recommendations are effective. Evaluate the performance of the learning model and the overall system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d3ea345"
      },
      "source": [
        "**Reasoning**:\n",
        "Design test cases, prepare input data, simulate data flow, recommendation generation, delivery, feedback submission, feedback ingestion, and model update, then evaluate performance. This involves multiple steps that can be grouped into a single simulation code block for testing the integrated system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2508ec88",
        "outputId": "c61790f9-28dd-48d4-9635-f940002be290"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import uuid # To generate unique recommendation IDs\n",
        "\n",
        "# Re-using the previously defined classes and handlers for the simulation\n",
        "# Assuming InputNodeDataHandler, CentralLearningSystem, and RecommendationOutput are defined in the kernel state\n",
        "\n",
        "# --- Step 1 & 2: Design Test Cases and Prepare Input Data ---\n",
        "\n",
        "# Test Case 1: Standard data flow and recommendation\n",
        "# Input: Normal data from an input node.\n",
        "# Expected: Central system processes data, generates recommendations, output node receives them.\n",
        "test_case_1_input = [\n",
        "    {'feature1': 5.5, 'feature2': 10.1, 'category': 'A', 'timestamp': '2024-01-01T10:00:00'},\n",
        "    {'feature1': 6.0, 'feature2': 11.5, 'category': 'B', 'timestamp': '2024-01-01T10:05:00'},\n",
        "]\n",
        "\n",
        "# Test Case 2: Data with missing values\n",
        "# Input: Data with some missing values.\n",
        "# Expected: Input handler cleans data, central system processes, recommendations generated.\n",
        "test_case_2_input = [\n",
        "    {'feature1': 7.1, 'feature2': 12.2, 'category': 'C', 'timestamp': '2024-01-01T10:10:00'},\n",
        "    {'feature1': 8.0, 'feature2': None, 'category': 'A', 'timestamp': '2024-01-01T10:15:00'},\n",
        "]\n",
        "\n",
        "# Test Case 3: Data with invalid types\n",
        "# Input: Data with invalid data types.\n",
        "# Expected: Input handler identifies and potentially rejects/cleans data.\n",
        "test_case_3_input = [\n",
        "    {'feature1': 9.5, 'feature2': 13.0, 'category': 'B', 'timestamp': '2024-01-01T10:20:00'},\n",
        "    {'feature1': 'bad_data', 'feature2': 14.5, 'category': 'C', 'timestamp': '2024-01-01T10:25:00'},\n",
        "]\n",
        "\n",
        "# Test Case 4: Feedback submission and model update\n",
        "# Input: Recommendations generated, output node provides feedback.\n",
        "# Expected: Central system receives feedback, updates model.\n",
        "# Data for this case is generated during the simulation flow.\n",
        "\n",
        "# --- Simulation Setup ---\n",
        "\n",
        "# Assuming col_types, input_dim, recommendation_outputs are defined from previous steps\n",
        "# col_types = {'feature1': 'float32', 'feature2': 'float32', 'category': 'object', 'timestamp': 'datetime64[ns]'} # Example\n",
        "# input_dim = 10 # Example\n",
        "# recommendation_outputs = 5 # Example\n",
        "\n",
        "# Initialize components\n",
        "input_handler = InputNodeDataHandler(list(col_types.keys()), col_types, fill_missing_value=0.0)\n",
        "central_system = CentralLearningSystem(input_shape=(input_dim,), num_recommendation_outputs=recommendation_outputs)\n",
        "central_system.compile_model() # Compile the model\n",
        "\n",
        "# Initial training of the central system (required before generating meaningful recommendations)\n",
        "print(\"Performing initial training of the central system...\")\n",
        "initial_train_data = np.random.rand(200, input_dim).astype(np.float32)\n",
        "initial_train_labels = np.random.rand(200, recommendation_outputs).astype(np.float32)\n",
        "central_system.train(initial_train_data, initial_train_labels, epochs=5)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Output node handler for simulation\n",
        "simulated_output_node_id = \"sim_output_node_001\"\n",
        "output_handler = RecommendationOutput(simulated_output_node_id)\n",
        "\n",
        "# Lists to store results and metrics\n",
        "test_results = {}\n",
        "feedback_data_log = []\n",
        "recommendation_log = []\n",
        "\n",
        "# --- Step 3-7: Simulate Data Flow, Recommendation, Delivery, Feedback ---\n",
        "\n",
        "print(\"--- Running Test Case 1: Standard Flow ---\")\n",
        "processed_data_tc1 = input_handler.receive_and_preprocess(test_case_1_input)\n",
        "if processed_data_tc1 is not None and not processed_data_tc1.empty:\n",
        "    # Simulate preparing data for the central system (e.g., feature extraction if needed)\n",
        "    # For this simple example, let's assume the processed_data_tc1 can be directly used as input features\n",
        "    # Need to ensure the shape matches the model's input_shape\n",
        "    # Dummy feature extraction/mapping to match input_dim\n",
        "    simulated_central_input_tc1 = processed_data_tc1[['feature1', 'feature2']].values # Use relevant numeric features\n",
        "    # Pad or process to match input_dim if necessary\n",
        "    if simulated_central_input_tc1.shape[1] < input_dim:\n",
        "        padding = np.zeros((simulated_central_input_tc1.shape[0], input_dim - simulated_central_input_tc1.shape[1]))\n",
        "        simulated_central_input_tc1 = np.hstack((simulated_central_input_tc1, padding))\n",
        "    simulated_central_input_tc1 = simulated_central_input_tc1.astype(np.float32)\n",
        "\n",
        "\n",
        "    recommendations_tc1 = central_system.generate_recommendations(simulated_central_input_tc1)\n",
        "    recommendation_log.append({\"test_case\": 1, \"recommendations\": recommendations_tc1})\n",
        "    output_handler.send_recommendations(recommendations_tc1)\n",
        "    test_results[\"test_case_1\"] = \"Success\"\n",
        "else:\n",
        "    test_results[\"test_case_1\"] = \"Failed: Data processing failed or empty\"\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "print(\"--- Running Test Case 2: Missing Values ---\")\n",
        "processed_data_tc2 = input_handler.receive_and_preprocess(test_case_2_input)\n",
        "if processed_data_tc2 is not None and not processed_data_tc2.empty:\n",
        "     # Simulate preparing data for the central system\n",
        "    simulated_central_input_tc2 = processed_data_tc2[['feature1', 'feature2']].values\n",
        "    if simulated_central_input_tc2.shape[1] < input_dim:\n",
        "        padding = np.zeros((simulated_central_input_tc2.shape[0], input_dim - simulated_central_input_tc2.shape[1]))\n",
        "        simulated_central_input_tc2 = np.hstack((simulated_central_input_tc2, padding))\n",
        "    simulated_central_input_tc2 = simulated_central_input_tc2.astype(np.float32)\n",
        "\n",
        "    recommendations_tc2 = central_system.generate_recommendations(simulated_central_input_tc2)\n",
        "    recommendation_log.append({\"test_case\": 2, \"recommendations\": recommendations_tc2})\n",
        "    output_handler.send_recommendations(recommendations_tc2)\n",
        "    test_results[\"test_case_2\"] = \"Success\"\n",
        "else:\n",
        "    test_results[\"test_case_2\"] = \"Failed: Data processing failed or empty\"\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "print(\"--- Running Test Case 3: Invalid Types ---\")\n",
        "processed_data_tc3 = input_handler.receive_and_preprocess(test_case_3_input)\n",
        "if processed_data_tc3 is not None and not processed_data_tc3.empty:\n",
        "     # Simulate preparing data for the central system\n",
        "    simulated_central_input_tc3 = processed_data_tc3[['feature1', 'feature2']].values\n",
        "    if simulated_central_input_tc3.shape[1] < input_dim:\n",
        "        padding = np.zeros((simulated_central_input_tc3.shape[0], input_dim - simulated_central_input_tc3.shape[1]))\n",
        "        simulated_central_input_tc3 = np.hstack((simulated_central_input_tc3, padding))\n",
        "    simulated_central_input_tc3 = simulated_central_input_tc3.astype(np.float32)\n",
        "\n",
        "    recommendations_tc3 = central_system.generate_recommendations(simulated_central_input_tc3)\n",
        "    recommendation_log.append({\"test_case\": 3, \"recommendations\": recommendations_tc3})\n",
        "    output_handler.send_recommendations(recommendations_tc3)\n",
        "    test_results[\"test_case_3\"] = \"Success\"\n",
        "else:\n",
        "    # Expected to fail or return empty due to invalid data, so mark as success if handled gracefully\n",
        "    test_results[\"test_case_3\"] = \"Handled Invalid Data (Expected)\"\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"--- Running Test Case 4: Feedback Loop ---\")\n",
        "# Generate recommendations first\n",
        "input_for_feedback_test = np.random.rand(1, input_dim).astype(np.float32)\n",
        "recommendations_for_feedback = central_system.generate_recommendations(input_for_feedback_test)\n",
        "output_handler.send_recommendations(recommendations_for_feedback)\n",
        "\n",
        "# Simulate feedback for the recommendations generated above\n",
        "# Let's assume the first recommendation was accepted, and the second rejected (if multiple)\n",
        "simulated_feedback_1 = {\n",
        "    'recommendation_id': str(uuid.uuid4()), # Unique ID for the recommendation event\n",
        "    'action': 'accepted',\n",
        "    'output_node': simulated_output_node_id,\n",
        "    'timestamp': pd.Timestamp.now().isoformat(),\n",
        "    'input_context': input_for_feedback_test.tolist(), # Include input context for potential retraining\n",
        "    'recommended_items': recommendations_for_feedback # Include recommendations sent\n",
        "}\n",
        "feedback_data_log.append(simulated_feedback_1)\n",
        "sent_feedback_1 = output_handler.send_feedback(simulated_feedback_1)\n",
        "central_system.receive_feedback(sent_feedback_1)\n",
        "\n",
        "# Simulate another feedback event\n",
        "simulated_feedback_2 = {\n",
        "    'recommendation_id': str(uuid.uuid4()),\n",
        "    'action': 'rejected',\n",
        "    'output_node': simulated_output_node_id,\n",
        "    'timestamp': pd.Timestamp.now().isoformat(),\n",
        "    'input_context': input_for_feedback_test.tolist(),\n",
        "    'recommended_items': recommendations_for_feedback\n",
        "}\n",
        "feedback_data_log.append(simulated_feedback_2)\n",
        "sent_feedback_2 = output_handler.send_feedback(simulated_feedback_2)\n",
        "central_system.receive_feedback(sent_feedback_2)\n",
        "\n",
        "\n",
        "# Process feedback and update the model\n",
        "print(\"Processing feedback and updating model...\")\n",
        "central_system.update_model_with_feedback(epochs=1)\n",
        "test_results[\"test_case_4\"] = \"Success\"\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- Step 8 & 9: Evaluate Performance ---\n",
        "\n",
        "print(\"--- Evaluating Performance ---\")\n",
        "\n",
        "# Model Performance Evaluation (Simplified)\n",
        "# In a real system, you would use a held-out test set and metrics like accuracy, precision, recall, RMSE.\n",
        "# Here, we can only observe the impact of feedback on recommendations qualitatively or track acceptance rate.\n",
        "\n",
        "# Qualitative check: Generate recommendations after feedback update and compare to before\n",
        "print(\"\\nRecommendations before feedback update (from Test Case 4):\")\n",
        "print(recommendations_for_feedback)\n",
        "print(\"\\nRecommendations after feedback update:\")\n",
        "recommendations_after_feedback_update = central_system.generate_recommendations(input_for_feedback_test)\n",
        "print(recommendations_after_feedback_update)\n",
        "\n",
        "# Track Acceptance Rate (Simple Simulation)\n",
        "# This requires analyzing the feedback_data_log\n",
        "accepted_count = sum(1 for fb in feedback_data_log if fb.get('action') == 'accepted')\n",
        "total_feedback = len(feedback_data_log)\n",
        "acceptance_rate = (accepted_count / total_feedback) if total_feedback > 0 else 0\n",
        "\n",
        "print(f\"\\nSimulated Acceptance Rate: {acceptance_rate:.2f}\")\n",
        "\n",
        "\n",
        "# Overall System Performance (Conceptual Evaluation)\n",
        "# Data Throughput: This simulation doesn't measure actual throughput but the handlers are designed for batch processing.\n",
        "# Latency: The simulation runs sequentially; real-world latency would depend on infrastructure and network.\n",
        "# Impact of Feedback: Qualitatively observed by comparing recommendations before and after the update. A real evaluation would track recommendation quality metrics over time as feedback is incorporated.\n",
        "\n",
        "print(\"\\n--- Test Results Summary ---\")\n",
        "for tc, status in test_results.items():\n",
        "    print(f\"{tc}: {status}\")\n",
        "\n",
        "print(\"\\n--- Feedback Log ---\")\n",
        "# display(pd.DataFrame(feedback_data_log)) # Display feedback log as a DataFrame if pandas is available\n",
        "\n",
        "print(\"\\n--- Recommendation Log (Sample) ---\")\n",
        "# Print a sample of recommendations generated\n",
        "for entry in recommendation_log[:2]: # Print first 2 entries\n",
        "    print(f\"Test Case {entry['test_case']}: {entry['recommendations']}\")\n",
        "\n",
        "\n",
        "# --- Step 10: Document Results ---\n",
        "# Results are documented via print statements and the test_results dictionary.\n",
        "# A more formal documentation would involve saving logs and metrics to files.\n",
        "\n",
        "print(\"\\n--- Simulation Finished ---\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing initial training of the central system...\n",
            "Starting model training...\n",
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - loss: 0.2655\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1211  \n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1100 \n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1058 \n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0995 \n",
            "Training finished.\n",
            "------------------------------\n",
            "RecommendationOutput initialized for node: sim_output_node_001\n",
            "--- Running Test Case 1: Standard Flow ---\n",
            "Scaled column: feature1\n",
            "Scaled column: feature2\n",
            "Generating raw recommendations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e03e0192de0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "Attempting to send recommendations to node: sim_output_node_001\n",
            "Format type: json\n",
            "Simulating sending JSON recommendations to sim_output_node_001:\n",
            "[[1, 0, 2, 3, 4], [4, 0, 3, 1, 2]]\n",
            "Simulation of sending recommendations to sim_output_node_001 finished.\n",
            "------------------------------\n",
            "--- Running Test Case 2: Missing Values ---\n",
            "Filled missing values with: 0.0\n",
            "Scaled column: feature1\n",
            "Scaled column: feature2\n",
            "Generating raw recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "Attempting to send recommendations to node: sim_output_node_001\n",
            "Format type: json\n",
            "Simulating sending JSON recommendations to sim_output_node_001:\n",
            "[[0, 4, 1, 3, 2], [0, 4, 1, 3, 2]]\n",
            "Simulation of sending recommendations to sim_output_node_001 finished.\n",
            "------------------------------\n",
            "--- Running Test Case 3: Invalid Types ---\n",
            "Error converting column 'feature1' to float32: could not convert string to float: 'bad_data'\n",
            "------------------------------\n",
            "--- Running Test Case 4: Feedback Loop ---\n",
            "Generating raw recommendations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e03e0192de0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "Attempting to send recommendations to node: sim_output_node_001\n",
            "Format type: json\n",
            "Simulating sending JSON recommendations to sim_output_node_001:\n",
            "[[3, 4, 2, 0, 1]]\n",
            "Simulation of sending recommendations to sim_output_node_001 finished.\n",
            "Attempting to send feedback from node: sim_output_node_001\n",
            "Feedback data: {'recommendation_id': 'e3af98b8-eb68-46d7-bbfe-b6cd6fbafe47', 'action': 'accepted', 'output_node': 'sim_output_node_001', 'timestamp': '2025-06-15T14:27:18.861260', 'input_context': [[0.27681663632392883, 0.9533751606941223, 0.3682761490345001, 0.6129206418991089, 0.8578522801399231, 0.007900971919298172, 0.38928863406181335, 0.34684211015701294, 0.6128790378570557, 0.12946195900440216]], 'recommended_items': [[3, 4, 2, 0, 1]]}\n",
            "Simulation of sending feedback from sim_output_node_001 finished.\n",
            "Central system received feedback.\n",
            "Current feedback buffer size: 1\n",
            "Attempting to send feedback from node: sim_output_node_001\n",
            "Feedback data: {'recommendation_id': '35113870-e0aa-4d5c-8d4b-ee760717af85', 'action': 'rejected', 'output_node': 'sim_output_node_001', 'timestamp': '2025-06-15T14:27:18.861708', 'input_context': [[0.27681663632392883, 0.9533751606941223, 0.3682761490345001, 0.6129206418991089, 0.8578522801399231, 0.007900971919298172, 0.38928863406181335, 0.34684211015701294, 0.6128790378570557, 0.12946195900440216]], 'recommended_items': [[3, 4, 2, 0, 1]]}\n",
            "Simulation of sending feedback from sim_output_node_001 finished.\n",
            "Central system received feedback.\n",
            "Current feedback buffer size: 2\n",
            "Processing feedback and updating model...\n",
            "Processing 2 feedback entries...\n",
            "Feedback processing finished. Buffer cleared.\n",
            "Preparing data for model update from feedback...\n",
            "Updating model with 2 feedback entries...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991ms/step - loss: 0.0971\n",
            "Model update based on feedback finished.\n",
            "------------------------------\n",
            "--- Evaluating Performance ---\n",
            "\n",
            "Recommendations before feedback update (from Test Case 4):\n",
            "[[3, 4, 2, 0, 1]]\n",
            "\n",
            "Recommendations after feedback update:\n",
            "Generating raw recommendations...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Post-processing raw predictions...\n",
            "Post-processing finished.\n",
            "[[3, 4, 2, 0, 1]]\n",
            "\n",
            "Simulated Acceptance Rate: 0.50\n",
            "\n",
            "--- Test Results Summary ---\n",
            "test_case_1: Success\n",
            "test_case_2: Success\n",
            "test_case_3: Handled Invalid Data (Expected)\n",
            "test_case_4: Success\n",
            "\n",
            "--- Feedback Log ---\n",
            "\n",
            "--- Recommendation Log (Sample) ---\n",
            "Test Case 1: [[1, 0, 2, 3, 4], [4, 0, 3, 1, 2]]\n",
            "Test Case 2: [[0, 4, 1, 3, 2], [0, 4, 1, 3, 2]]\n",
            "\n",
            "--- Simulation Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ed2d8f"
      },
      "source": [
        "## Deployment and monitoring\n",
        "\n",
        "### Subtask:\n",
        "Deploy the architecture to a distributed environment and set up monitoring to track its performance and identify any issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badf7d36"
      },
      "source": [
        "**Reasoning**:\n",
        "Outline a deployment strategy for each component in a distributed environment using containerization and orchestration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e66dc17",
        "outputId": "6522e3e1-d4e4-4f60-b84b-f14f75cc5de2"
      },
      "source": [
        "# 1. Deployment Strategy Outline\n",
        "\n",
        "# General Approach: Containerization with Docker and Orchestration with Kubernetes\n",
        "\n",
        "# Input Nodes:\n",
        "# - Containerization: Each type of input node (or instances of the same type) will be packaged into Docker containers. This ensures consistency across different deployment environments and simplifies dependencies.\n",
        "# - Deployment: Deploy input node containers as Kubernetes Pods. These pods can run on edge devices (if applicable) or within the cloud environment close to the data source.\n",
        "# - Scaling: Use Kubernetes Deployments to manage the input node pods. Scaling can be based on metrics like data volume or processing load using Horizontal Pod Autoscalers (HPA).\n",
        "# - Configuration Management: Use Kubernetes ConfigMaps or Secrets to manage configuration specific to each input node (e.g., connection strings, API keys).\n",
        "\n",
        "# Central Learning System:\n",
        "# - Containerization: Package the central learning system (including the model training and serving components) into one or more Docker containers. Separate containers might be used for different functionalities (e.g., data ingestion processing, model training jobs, recommendation serving API).\n",
        "# - Deployment: Deploy the central system components as Kubernetes Deployments.\n",
        "# - Scaling:\n",
        "#   - Recommendation Serving: Use HPA to scale the recommendation serving pods based on request load (CPU utilization, custom metrics).\n",
        "#   - Data Processing/Training: Use Kubernetes Jobs or StatefulSets for batch processing or stateful components. Scaling for these might be based on queue size (e.g., Kafka queue for incoming data/feedback).\n",
        "# - High Availability: Use multiple replicas for stateless components (recommendation serving) and consider StatefulSets with persistent storage for stateful components or databases.\n",
        "# - Configuration Management: Use ConfigMaps and Secrets for database credentials, cloud service API keys, model configuration, etc.\n",
        "\n",
        "# Output Nodes:\n",
        "# - Containerization: If output nodes are software applications, they can be containerized using Docker. For physical devices, this step is not applicable in the same way, but client software on the device can be managed.\n",
        "# - Deployment: Deploy output node containers as Kubernetes Pods, potentially in a different namespace or cluster depending on the architecture. For edge devices, consider Kubernetes distributions like K3s or MicroK8s.\n",
        "# - Scaling: Scale output node pods based on the number of users or devices they serve.\n",
        "# - Configuration Management: Use ConfigMaps or Secrets for configuration like the central system's API endpoint.\n",
        "\n",
        "# Communication Infrastructure (e.g., Kafka, Databases):\n",
        "# - Deployment: Deploy communication components (like Kafka brokers, database clusters) within the Kubernetes cluster or as managed services in the cloud (e.g., Confluent Cloud, AWS RDS, Azure Cosmos DB).\n",
        "# - High Availability: Ensure these critical components are deployed with high availability configurations (e.g., multiple replicas, multi-AZ deployments).\n",
        "# - Scaling: Scale these components based on throughput and storage requirements.\n",
        "\n",
        "print(\"Deployment strategy outlined.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deployment strategy outlined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ee4b42"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the key metrics to monitor for each component and identify tools and technologies for monitoring and logging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76950331",
        "outputId": "d2f0655e-3f85-4916-d0c9-354664d6dda5"
      },
      "source": [
        "# 2. Key Metrics to Monitor\n",
        "\n",
        "# Input Nodes:\n",
        "# - Data Ingestion Rate: Number of records/bytes processed per unit of time.\n",
        "# - Data Validation Errors: Count of records that failed validation checks.\n",
        "# - Preprocessing Latency: Time taken to preprocess a batch of data.\n",
        "# - Connection Status: Status of connection to the central system's ingestion endpoint (e.g., Kafka broker).\n",
        "# - Resource Utilization: CPU, memory, network bandwidth usage.\n",
        "\n",
        "# Central Learning System:\n",
        "# - Data Ingestion Rate: Rate of data received from input nodes.\n",
        "# - Feedback Ingestion Rate: Rate of feedback received from output nodes.\n",
        "# - Data Processing Latency: Time taken to process incoming data batches for training/inference.\n",
        "# - Model Training Metrics: Loss, accuracy (or other relevant metrics) per epoch/batch during training.\n",
        "# - Model Update Frequency/Latency: How often the model is updated and how long the update process takes.\n",
        "# - Recommendation Inference Latency: Time taken to generate recommendations for a request.\n",
        "# - Error Rates: Rate of errors during data processing, model training, or recommendation generation.\n",
        "# - Resource Utilization: CPU, GPU, memory, network, storage I/O usage.\n",
        "# - Queue Sizes: Size of internal queues for data ingestion, feedback, or processing tasks.\n",
        "\n",
        "# Output Nodes:\n",
        "# - Recommendation Reception Rate: Rate of recommendations received.\n",
        "# - Recommendation Display Latency: Time taken to display/act on a recommendation after receiving it.\n",
        "# - Feedback Submission Rate: Rate of feedback sent to the central system.\n",
        "# - Action Rate: Rate at which recommendations are acted upon (if applicable).\n",
        "# - Error Rates: Errors during recommendation reception or feedback submission.\n",
        "# - Resource Utilization: CPU, memory, network usage.\n",
        "\n",
        "# Communication Infrastructure (Kafka, Databases):\n",
        "# - Throughput: Data ingress/egress rate.\n",
        "# - Latency: Message delivery latency.\n",
        "# - Error Rates: Connection errors, production/consumption failures.\n",
        "# - Resource Utilization: Broker/database node CPU, memory, storage, network usage.\n",
        "# - Partition/Shard Health: Status and load distribution.\n",
        "\n",
        "# 3. Monitoring and Logging Tools and Technologies\n",
        "\n",
        "# - Monitoring:\n",
        "# - Prometheus: Open-source monitoring and alerting system. Can collect metrics from applications (via client libraries) and infrastructure.\n",
        "# - Grafana: Open-source data visualization and dashboarding tool. Integrates well with Prometheus and other data sources to visualize metrics.\n",
        "# - Cloud-specific Monitoring Services: AWS CloudWatch, Azure Monitor, Google Cloud Monitoring. Provide integrated monitoring for cloud resources and applications.\n",
        "# - Custom Metrics: Instrument application code with client libraries (e.g., Prometheus client libraries for Python) to expose custom metrics like processing latency, queue sizes, or application-specific error counts.\n",
        "\n",
        "# - Logging:\n",
        "# - ELK Stack (Elasticsearch, Logstash, Kibana): Popular open-source platform for collecting, processing, storing, and analyzing log data.\n",
        "# - Fluentd/Fluent Bit: Lightweight log processors and forwarders. Can collect logs from various sources and send them to Elasticsearch or other destinations.\n",
        "# - Cloud-specific Logging Services: AWS CloudWatch Logs, Azure Monitor Logs, Google Cloud Logging. Centralized logging services that integrate with other cloud services.\n",
        "# - Structured Logging: Implement structured logging within applications (e.g., using Python's `logging` module with a formatter) to make logs easier to parse and analyze.\n",
        "\n",
        "# - Tracing:\n",
        "# - Jaeger or Zipkin: Distributed tracing systems to visualize request flows across different services and identify bottlenecks. Useful for debugging latency issues in a distributed system.\n",
        "# - OpenTelemetry: A set of APIs, SDKs, and tools to instrument, generate, collect, and export telemetry data (metrics, logs, traces).\n",
        "\n",
        "print(\"Key metrics and monitoring/logging tools described.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key metrics and monitoring/logging tools described.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5bafc3"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain how alerts would be configured based on the monitored metrics to detect and respond to issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ccd215",
        "outputId": "b398245b-a3ee-4841-d850-633b9ee21b55"
      },
      "source": [
        "# 4. Alert Configuration\n",
        "\n",
        "# Alerts are crucial for proactive issue detection and response. They are typically configured based on thresholds or patterns in the monitored metrics.\n",
        "\n",
        "# Alerting Tools: Prometheus Alertmanager (often used with Prometheus), cloud-specific alerting services (AWS CloudWatch Alarms, Azure Monitor Alerts, Google Cloud Monitoring Alerting).\n",
        "\n",
        "# Examples of Alert Configurations:\n",
        "\n",
        "# - Input Node Alert:\n",
        "#   - Metric: `input_node_data_ingestion_rate`\n",
        "#   - Condition: `rate(input_node_data_ingestion_rate[5m]) < 10` (average ingestion rate over 5 minutes drops below 10 records/sec)\n",
        "#   - Severity: Warning\n",
        "#   - Action: Notify relevant team, potentially trigger investigation into the input node's health or data source.\n",
        "\n",
        "# - Central Learning System Alert:\n",
        "#   - Metric: `central_system_recommendation_inference_latency`\n",
        "#   - Condition: `central_system_recommendation_inference_latency > 500ms` (recommendation latency exceeds 500 milliseconds)\n",
        "#   - Severity: Critical\n",
        "#   - Action: Notify on-call engineers, potentially trigger scaling up of recommendation serving pods or investigate underlying causes (e.g., database performance).\n",
        "\n",
        "# - Central Learning System Alert (Model Training):\n",
        "#   - Metric: `central_system_model_training_loss`\n",
        "#   - Condition: `central_system_model_training_loss > 0.1` (training loss is unexpectedly high after an update)\n",
        "#   - Severity: Warning/Critical (depending on the model and threshold)\n",
        "#   - Action: Notify ML engineers, investigate the new data, model convergence, or training pipeline.\n",
        "\n",
        "# - Output Node Alert:\n",
        "#   - Metric: `output_node_feedback_submission_error_rate`\n",
        "#   - Condition: `sum(rate(output_node_feedback_submission_errors[5m])) by (output_node_id) > 0` (an output node is experiencing feedback submission errors)\n",
        "#   - Severity: Warning\n",
        "#   - Action: Notify support team or the team responsible for that output node, investigate connectivity or data issues.\n",
        "\n",
        "# - Communication Infrastructure Alert (Kafka):\n",
        "#   - Metric: `kafka_broker_messages_in_per_sec`\n",
        "#   - Condition: `rate(kafka_broker_messages_in_per_sec[5m]) < 1000` (total messages ingested into Kafka drops significantly)\n",
        "#   - Severity: Critical\n",
        "#   - Action: Notify infrastructure team, investigate Kafka cluster health or upstream producers.\n",
        "\n",
        "# - Resource Utilization Alert:\n",
        "#   - Metric: `kubernetes_pod_cpu_utilization` (for a specific deployment, e.g., central system serving pods)\n",
        "#   - Condition: `avg(kubernetes_pod_cpu_utilization{deployment=\"recommendation-serving\"}) by (pod) > 80%` (average CPU utilization for serving pods exceeds 80%)\n",
        "#   - Severity: Warning\n",
        "#   - Action: Notify infrastructure team, potentially trigger scaling actions if HPA is not configured or effective.\n",
        "\n",
        "# Alerting Workflow:\n",
        "# 1. Monitoring tool detects a metric violating a configured condition.\n",
        "# 2. An alert is triggered and sent to the alerting tool (e.g., Alertmanager).\n",
        "# 3. The alerting tool groups similar alerts and routes them to the appropriate notification channels (e.g., Slack, PagerDuty, email).\n",
        "# 4. On-call personnel or relevant teams receive the alert and take action to investigate and resolve the issue.\n",
        "\n",
        "# Best Practices:\n",
        "# - Define clear, actionable alerts.\n",
        "# - Use appropriate severity levels.\n",
        "# - Configure notification routing based on alert severity and component ownership.\n",
        "# - Regularly review and tune alerts to reduce noise.\n",
        "# - Implement runbooks or documentation for responding to common alerts.\n",
        "\n",
        "print(\"Alert configuration explained.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alert configuration explained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a62862f"
      },
      "source": [
        "## Refine and iterate\n",
        "\n",
        "### Subtask:\n",
        "Refine and iterate on the architecture, learning models, and recommendation engine based on performance data and feedback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c957590"
      },
      "source": [
        "**Reasoning**:\n",
        "Analyze the performance data and feedback, identify areas for improvement, propose concrete changes, and document the rationale. This involves iterating on the existing components based on the simulated test results and feedback logs. Since the previous steps involved creating the components and running a basic simulation, this step will analyze the outcomes of that simulation (as documented in the test_results and feedback_data_log variables) and propose refinements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4924f427",
        "outputId": "d187ea14-d84d-4df5-c419-883fca4059c2"
      },
      "source": [
        "# 1. Analyze Performance Metrics and Feedback\n",
        "print(\"--- Analyzing Performance Metrics and Feedback ---\")\n",
        "\n",
        "# Analyze Test Results\n",
        "print(\"\\nTest Results Summary:\")\n",
        "for tc, status in test_results.items():\n",
        "    print(f\"{tc}: {status}\")\n",
        "# Observations:\n",
        "# - Test Case 1 (Standard Flow): Success - Basic data ingestion, recommendation, and delivery worked.\n",
        "# - Test Case 2 (Missing Values): Success - Input handler correctly filled missing values.\n",
        "# - Test Case 3 (Invalid Types): Handled Invalid Data (Expected) - Input handler correctly identified and stopped processing invalid data.\n",
        "# - Test Case 4 (Feedback Loop): Success - Feedback was received and the model update process was initiated.\n",
        "\n",
        "# Analyze Feedback Data (from feedback_data_log)\n",
        "print(\"\\nFeedback Data Log (Sample):\")\n",
        "if feedback_data_log:\n",
        "    for entry in feedback_data_log[:5]: # Display first 5 entries\n",
        "        print(entry)\n",
        "else:\n",
        "    print(\"No feedback data collected during simulation.\")\n",
        "\n",
        "# Calculate Acceptance Rate (already done in previous step, just printing)\n",
        "print(f\"\\nSimulated Acceptance Rate: {acceptance_rate:.2f}\")\n",
        "# Observations:\n",
        "# - Acceptance rate is 0.5, based on one accepted and one rejected feedback. This is from a very small sample.\n",
        "# - Feedback structure includes 'recommendation_id', 'action', 'output_node', 'timestamp', 'input_context', 'recommended_items'. This provides rich context for learning.\n",
        "\n",
        "# Analyze Recommendation Changes after Feedback (Qualitative)\n",
        "# As noted in the previous step, recommendations before and after the feedback update were the same\n",
        "print(\"\\nQualitative Analysis of Recommendation Changes After Feedback:\")\n",
        "print(\"In the simulation, recommendations before and after a single feedback-based update were identical.\")\n",
        "# Observation:\n",
        "# - The simplified model update based on feedback (using random data/labels) was not sufficient to cause a noticeable change in recommendations with only two feedback entries and 1 epoch of training.\n",
        "# - This highlights the need for a more sophisticated feedback integration strategy and potentially more data/training.\n",
        "\n",
        "\n",
        "# 2. Identify Specific Areas for Improvement\n",
        "\n",
        "print(\"\\n--- Identifying Areas for Improvement ---\")\n",
        "\n",
        "# Based on analysis:\n",
        "# a. Feedback Integration and Model Update: The current `update_model_with_feedback` is a placeholder and needs a realistic implementation. The current simulation shows it doesn't effectively change recommendations with minimal feedback.\n",
        "# b. Data Diversity and Volume: The simulation used small amounts of dummy data. Real-world data diversity and volume will impact model performance and training strategy.\n",
        "# c. Evaluation Metrics: The simulation used simplified evaluation (test case status, basic acceptance rate). A real system needs more comprehensive metrics for model performance (e.g., offline evaluation on historical data) and online A/B testing or interleaved experiments for recommendation effectiveness.\n",
        "# d. Input Data Representation: The simple concatenation of 'feature1' and 'feature2' for the central system input in the simulation is likely insufficient for a real model. More sophisticated feature engineering from diverse input data types is needed.\n",
        "# e. Handling Different Feedback Types: The current feedback handles 'accepted'/'rejected' actions. Real systems might have different types of feedback (ratings, dwell time, conversions).\n",
        "# f. Scalability: The current implementation is sequential. A real system needs to handle concurrent data streams and recommendation requests. (Addressed conceptually in deployment, but implementation details matter).\n",
        "\n",
        "\n",
        "# 3. Propose Concrete Changes\n",
        "\n",
        "print(\"\\n--- Proposed Concrete Changes ---\")\n",
        "\n",
        "# a. Enhance Feedback Integration and Model Update:\n",
        "#    - Modify `CentralLearningSystem.process_feedback` to transform raw feedback into structured training data. This will likely involve joining feedback with the input context that led to the recommendation.\n",
        "#    - Modify `CentralLearningSystem.update_model_with_feedback` to train or fine-tune the model using this structured feedback data. Consider techniques like:\n",
        "#      - Re-ranking: Use feedback to adjust the scores or rankings of recommended items.\n",
        "#      - Policy Learning (Reinforcement Learning): Treat recommendation as an action and feedback as a reward signal to train a recommendation policy.\n",
        "#      - Weighted Retraining: Assign higher weights to recent or high-value feedback during incremental training.\n",
        "#    - Implement a mechanism to periodically process accumulated feedback and trigger model updates.\n",
        "\n",
        "# b. Improve Input Data Representation:\n",
        "#    - Refine `InputNodeDataHandler.receive_and_preprocess` and the simulation logic to map diverse input features to a suitable input representation for the model (e.g., using embedding layers for categorical features, handling time series data appropriately). The current dummy mapping from ['feature1', 'feature2'] to a fixed-size vector needs to be generalized.\n",
        "\n",
        "# c. Implement Comprehensive Evaluation Metrics:\n",
        "#    - Add logic for offline evaluation in the `CentralLearningSystem` or a separate module using historical data with known outcomes. Calculate metrics like Precision@K, Recall@K, Mean Average Precision (MAP), RMSE (for regression outputs).\n",
        "#    - Plan for online evaluation strategies (A/B testing) in the deployment phase to measure the impact of new model versions or recommendation logic in a live environment.\n",
        "\n",
        "# d. Generalize Feedback Processing:\n",
        "#    - Update `CentralLearningSystem.receive_feedback` and `process_feedback` to handle different feedback types and incorporate them into the learning process appropriately.\n",
        "\n",
        "# e. Refine Model Architecture:\n",
        "#    - Based on the improved input data representation, the model architecture (`CentralLearningSystem._build_model`) might need to be adjusted (e.g., adding different input branches for different data types, using more complex layers like LSTMs or Transformers if dealing with sequential data).\n",
        "\n",
        "# 4. Document Proposed Changes and Rationale\n",
        "\n",
        "print(\"\\n--- Documentation of Proposed Changes and Rationale ---\")\n",
        "\n",
        "# Change 1: Enhance Feedback Integration and Model Update\n",
        "# Rationale: The current feedback integration is a placeholder. A proper implementation is needed to leverage feedback signals (acceptance/rejection) to improve recommendation quality over time. Using techniques like re-ranking or reinforcement learning can directly optimize for user engagement or satisfaction.\n",
        "\n",
        "# Change 2: Improve Input Data Representation\n",
        "# Rationale: The current simulation uses a simplistic mapping of input features. Real-world data is diverse, and the model's performance heavily depends on how effectively input data is represented as features. This requires proper feature engineering and potentially a more complex model input layer.\n",
        "\n",
        "# Change 3: Implement Comprehensive Evaluation Metrics\n",
        "# Rationale: Relying solely on test case status and basic acceptance rate is insufficient to assess the effectiveness of recommendations and the learning system's performance. Comprehensive offline and online metrics are necessary to track progress, compare different model versions, and make data-driven decisions for iteration.\n",
        "\n",
        "# Change 4: Generalize Feedback Processing\n",
        "# Rationale: Different types of feedback provide different signals about recommendation quality. Generalizing the feedback processing allows the system to utilize richer user interaction data for learning.\n",
        "\n",
        "# Change 5: Refine Model Architecture\n",
        "# Rationale: The initial model is a simple feed-forward network. As the input data representation improves and the complexity of the recommendation task is fully understood, the model architecture needs to evolve to effectively learn from the data and generate better recommendations. This might involve incorporating domain-specific architectural patterns.\n",
        "\n",
        "print(\"\\nAnalysis, identification of improvements, and proposed changes documented.\")\n",
        "\n",
        "# The subtask is completed."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Analyzing Performance Metrics and Feedback ---\n",
            "\n",
            "Test Results Summary:\n",
            "test_case_1: Success\n",
            "test_case_2: Success\n",
            "test_case_3: Handled Invalid Data (Expected)\n",
            "test_case_4: Success\n",
            "\n",
            "Feedback Data Log (Sample):\n",
            "{'recommendation_id': 'e3af98b8-eb68-46d7-bbfe-b6cd6fbafe47', 'action': 'accepted', 'output_node': 'sim_output_node_001', 'timestamp': '2025-06-15T14:27:18.861260', 'input_context': [[0.27681663632392883, 0.9533751606941223, 0.3682761490345001, 0.6129206418991089, 0.8578522801399231, 0.007900971919298172, 0.38928863406181335, 0.34684211015701294, 0.6128790378570557, 0.12946195900440216]], 'recommended_items': [[3, 4, 2, 0, 1]]}\n",
            "{'recommendation_id': '35113870-e0aa-4d5c-8d4b-ee760717af85', 'action': 'rejected', 'output_node': 'sim_output_node_001', 'timestamp': '2025-06-15T14:27:18.861708', 'input_context': [[0.27681663632392883, 0.9533751606941223, 0.3682761490345001, 0.6129206418991089, 0.8578522801399231, 0.007900971919298172, 0.38928863406181335, 0.34684211015701294, 0.6128790378570557, 0.12946195900440216]], 'recommended_items': [[3, 4, 2, 0, 1]]}\n",
            "\n",
            "Simulated Acceptance Rate: 0.50\n",
            "\n",
            "Qualitative Analysis of Recommendation Changes After Feedback:\n",
            "In the simulation, recommendations before and after a single feedback-based update were identical.\n",
            "\n",
            "--- Identifying Areas for Improvement ---\n",
            "\n",
            "--- Proposed Concrete Changes ---\n",
            "\n",
            "--- Documentation of Proposed Changes and Rationale ---\n",
            "\n",
            "Analysis, identification of improvements, and proposed changes documented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c6b192"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The distributed architecture was successfully defined, outlining the roles and responsibilities of Input Nodes, Output Nodes, and the Central Learning/Recommendation System.\n",
        "*   The data flow within the system, including data ingestion, processing, recommendation generation, delivery, and the feedback loop, was mapped out.\n",
        "*   Suitable technologies for each component and communication channel were identified, considering aspects like distributed computing, data processing, ML frameworks, and protocols like Kafka, gRPC, and REST.\n",
        "*   A basic implementation for input data ingestion and preprocessing was developed, demonstrating handling of missing values and invalid data types.\n",
        "*   The core of the central learning system, including a simple neural network model for generating recommendations and methods for training and updating, was implemented.\n",
        "*   The logic for generating recommendations, including a basic post-processing step (ranking), was developed.\n",
        "*   A mechanism for delivering recommendations to output nodes and simulating feedback submission was implemented.\n",
        "*   A simulation of the integrated system demonstrated the basic data flow, recommendation generation, and feedback loop, although the impact of feedback on the simplified model was not immediately observable.\n",
        "*   A strategy for deploying the system using Docker and Kubernetes, identifying key monitoring metrics, and setting up logging and alerting was outlined.\n",
        "*   Analysis of the simulated results and feedback identified areas for improvement, particularly in the feedback integration mechanism and input data representation, leading to proposed refinements for future iterations.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current feedback integration and model update mechanism is a simplified placeholder and needs to be significantly enhanced to effectively leverage user feedback for improving recommendation quality over time.\n",
        "*   Future iterations should focus on implementing more sophisticated feature engineering for diverse input data, developing comprehensive evaluation metrics (both offline and online), and refining the model architecture based on real-world data characteristics and feedback signals.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "functional_api",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}